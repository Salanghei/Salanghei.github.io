<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《Java编程思想》读书笔记（一）]]></title>
    <url>%2F2019%2F10%2F28%2F%E3%80%8AJava%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[第二章 一切都是对象用句柄操纵对象尽管将一切都“看作”对象，但操纵的标识符实际是指向一个对象的“句柄”（Handle） 所有对象都必须创建 字符串的特殊初始化方式：用加引号的文字初始化 数据的保存位置： 寄存器（处理器内部）：最快的保存区域，根据需要由编译器分配 堆栈（RAM中，随机访问存储器）：创建程序时，Java 编译器必须准确地知道堆栈内保存的所有数据的“长度”以及“存在时间”，有些 Java 数据要保存在堆栈里——特别是对象句柄，但Java 对象并不放到其中 堆（RAM中）：保存Java对象，比堆栈更灵活，但在堆里分配存储空间时会花掉更长的时间 静态存储（RAM中）：指“位于固定位置”，可用static 关键字指出一个对象的特定元素是静态的，但 Java 对象本身永远都不会置入静态存储空间 常数存储：直接置于代码内部，有的常数需要严格地保护，所以可考虑将它们置入只读存储器（ROM） 非RAM存储器：“流式对象”和“固定对象”（磁盘中） 主要类型：置于堆栈中，更高效的存取 Java数组：安全性，保证被初始化，而且不可在它的范围之外访问对象数组：实际创建的是一个句柄数组。而且每个句柄都会自动初始化成一个特殊值，并带有自己的关键字：null（空）主类型数组：将数组的内存划分成零 绝对不要清除对象 作用域：同时决定了它的“可见性”以及“存在时间” Java 有一个特别的“垃圾收集器”，它会查找用new 创建的所有对象，并辨别其中哪些不再被引用。随后，它会自动释放由那些闲置对象占据的内存，以便能由新对象使用 新建数据类型：类主成员的默认值：若某个主数据类型属于一个类成员，那么即使不明确（显式）进行初始化，也可以保证它们获得一个默认值，这种保证却并不适用于“局部”变量——那些变量并非一个类的字段 方法、自变量和返回值 Java 的方法只能作为类的一部分创建 “静态”方法可针对类调用，毋需一个对象 自变量列表：实际传递的是“句柄”（对于前面提及的“特殊”数据类型 boolean，char，byte，short，int，long，float 以及double来说是一个例外。但在传递对象时，通常都是指传递指向对象的句柄） 构建Java程序 static关键字：一旦将什么东西设为static，数据或方法就不会同那个类的任何对象实例联系到一起，因为引用的是同样的内存区域，对方法来说，static 一项重要的用途就是帮助我们在不必创建对象的前提下调用那个方法 一个特殊的类库会自动导入每个Java 文件：java.lang main函数中的args保存了在命令行调用的自变量 第三章 控制程序流程使用Java运算符 几乎所有运算符都只能操作“主类型”（Primitives）。唯一的例外是“=”、“==”和“!=”，它们能操作所有对象（也是对象易令人混淆的一个地方）。除此以外，String 类支持“+”和“+=” 主类型的赋值：由于主类型容纳了实际的值，而且并非指向一个对象的句柄，所以在为其赋值的时候，可将来自一个地方的内容复制到另一个地方对象的赋值：复制句柄，而两个句柄指向堆中同一个对象 自动递增和递减：对于前递增和前递减（如++A 或–A），会先执行运算，再生成值。而对于后递增和后递减（如A++或A–），会先生成值，再执行运算 “==”比较的是对象句柄，equals()比较的是两个对象的实际内容（Java类库中的类），但如果是自己创建的类，equals()的默认行为是比较句柄，如需比较内容，需要在该类中修改equals() 逻辑运算符中的“短路”：逻辑表达式的任意部分都有可能不进行求值 按位运算符会对两个自变量中对应的位执行布尔代数，并最终生成一个结果 移位运算符：“有符号”右移位运算符使用了“符号扩展”：若值为正，则在高位插入 0；若值为负，则在高位插入1。“无符号”右移位运算符（&gt;&gt;&gt;）使用了“零扩展”：无论正负，都在高位插入0 “造型”运算符（强制类型转换）：boolean类型不能执行该运算 执行控制 只有 for 循环才具备在控制表达式里定义变量的能力。对于其他任何条件或循环语句，都不可采用这种方法 标签：对Java 来说，唯一用到标签的地方是在循环语句之前，它需要紧靠在循环语句的前方，设置标签的唯一理由是：我们希望在其中嵌套另一个循环。由于 break 和 continue 关键字通常只中断当前循环，但若随同标签使用，它们就会中断到存在标签的地方 switch语句要求使用一个选择因子，并且必须是 int 或char 那样的整数值 第四章 初始化和清除方法重载 区分重载的方法：每个过载的方法都必须采取独一无二的自变量类型列表，不能根据返回值类型来区分过载的方法 如果在类中已经定义了一个构建器（无论是否有自变量），编译程序不会再帮我们自动生成一个默认构建起 this关键字：在一个方法的内部希望获得当前对象的句柄。通常，当我们说this 的时候，都是指“这个对象”或者“当前对象”，而且它本身会产生当前对象的一个句柄 在构建器中调用构建器：不可以同时调用两个构建器；构建器必须是执行的第一条语句；在类的内部不允许除构建器之外的任何方法调用构建器static关键字：不可从static方法内部访问非static成员 清除：收尾和垃圾回收（？）垃圾收收集器（GC）只知道释放由new关键字分配的内存，所以不知道如何释放对象的“特殊”内存。为了解决这个问题，Java提供了一个名为finalize()的方法。如果定义了finalize()方法，那么java在回收该类的一个对象时就会调用这个方法。在finalize()中要指定回收前要进行的操作。finalize正好在垃圾回收之前被调用，也就一样的具有了时间的不确定性 成员初始化 “向前引用”，注意初始化顺序，且变量的初始化会在调用任何方法（包括构建器）之前执行，自动初始化（将变量初始化为默认值）在构建器执行初始化之前执行 static对象初始化：在创建某个类的第一个对象时初始化，后期再创建该类的对象时不会重新初始化，且static对象初始化在非static对象初始化之前]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Providing Explanation for Recommendations in Reciprocal Environments]]></title>
    <url>%2F2019%2F10%2F17%2FProviding-Explanation-for-Recommendations-in-Reciprocal-Environments%2F</url>
    <content type="text"><![CDATA[介绍互惠环境（REs，Reciprocal Environments）：在推荐时需要综合考虑推荐接收用户的偏好和被推荐用户的偏好，像求职类、约会类的应用，这类的平台或应用称为互惠环境互惠推荐系统（RRSs，Reciprocal Recommender Systems）：在互惠环境中为用户找到合适的匹配，称为互惠推荐系统已经有研究工作证实，同时考虑双方的偏好的推荐方法，比只考虑推荐接收用户的偏好的推荐方法，更适合互惠环境 新的问题出现：RRS如何解释推荐原因传统的解释方法（仅考虑推荐接收用户的偏好）已被证实，可以在非互惠环境中提升用户接受度（user’s acceptance rate）、用户主观满意度（user’s subjective satisfaction）、用户对系统的信任度（user’s trust），但是否适用于互惠环境尚不清楚 本文的主要工作提出并评估了一种新的基于推荐双方偏好的解释方法，互惠解释（reciprocal explanations）针对在线约会应用，分别在模拟数据和真实数据上进行实验，将互惠解释与传统解释方法进行对比，证明当推荐的是否接受被关联到某个权重时（发送私信花费的时间权重、担心被拒绝的感情权重等），互惠解释方法明显优于传统解释方法，因为此时提供互惠的解释，会提高用户对推荐的接受度和对系统的信任度；但如果这个权重可以忽略时，传统解释方法要优于互惠解释法 相关工作和背景可解释人工智能（XAI，Explainable Artificial Intelligence）是一个新兴的领域，其目的是使自动化系统可理解。有大量的为推荐生成解释的方法已经被提出。在推荐领域中常伴有以下两个习惯：1）已有的解释方法仅关注了推荐接受者2）已有的解释方法与特定的应用或特定的算法有很强的依赖，不能在不同的领域中很好的适用 许多研究也证实了为推荐提供解释是有好处的，结合许多研究工作，总结出两个在研究解释方法时被广泛认可的原则：1）对被推荐用户/项目（item）具体特征的解释，会使推荐更高效，即使这些特征可能不是生成推荐的原因2）限制解释的容量很重要，信息过载会适得其反 在线约会的推荐算法本文的工作针对在线约会领域，主要研究解释的生成，使用在这个领域最先进的两个算法：RECON和双向协同过滤（Two-sided collaborative filtering） RECON算法用户x由两个成分定义：1）A_x={v_a}，用户x的个人属性值列表，v_a 是用户x对应a属性的属性值2）p_(x,a)={(v_a, n)}，用户x的偏好列表，n为用户x发送给a属性值为v_a 的用户的消息数量 e.g. Bob是一个男性用户，他给10个不同的女性发送过消息，假设用两个属性值描述一个用户：吸烟习惯（smoke habits）和身材（body type）。Bob发送信息的女性中，吸烟习惯的分布：1个经常吸烟（regularly），3个偶尔吸烟（occasionally），6个从不吸烟（never）；身材的分布：4个苗条（slim），4个平均（average），2个强壮（athletic）。所以Bob的偏好如下：p_(Bob,smoke)={(1,regular), (3,occasionally),(6,never)}p_(Bob,body−type)={(4,slim), (4,average),(2,athletic)}RECON算法通过启发函数预测每对用户x,y间的偏好，启发函数反应了两个用户间各自的偏好和对方的属性是如何校准的 双向协同过滤（Two-sided collaborative filtering）双向协同过滤算法的性能优于RECON算法相似度：根据历史消息计算用户间的相似度，如果两个用户都有大部分消息是发送给同一用户的，则认为两个用户是相似的如果算法要向用户x推荐用户y：首先通过比较x与其他向y发送过消息的用户之间的相似度，得到x对y的感兴趣程度，然后再对称的计算y对x的感兴趣程度，最后将二者合并到一个单一度量中，表示x与y互相的感兴趣程度的匹配，最后再使用协同过滤进行推荐 生成互惠解释假设根据上述算法为用户x推荐了用户y，将涵盖了用户x对y的兴趣的解释称为单向解释（one-sided explanation），记为e_(x,y)将涵盖了用户x对y的兴趣，以及用户y对x的兴趣的解释称为互惠解释，互惠解释可以分解为一对单向解释，e_(x,y) 和e_(y,x) 实验研究本文设计了3个实验，其中2个实验在模拟环境中进行，1个实验在真实环境中进行 MATCHMAKER模拟环境（简称MM）在MM环境中，用户可以看到他人的基本信息，并与他人通过消息沟通为了使MM中的用户基本信息尽可能真实，我们使用真实在线约会网站中的属性，但是这里的基本信息不包含用户的历史消息和偏好信息收集：招募121位参与者，其中63名男性，58名女性，年龄在18到35岁之间（平均23.3岁），每个参与者都是单身的异性恋者。参与者首先进入MM系统，填写基本信息，然后看从真实网站中获取到的用户基本资料，并向他们 觉得合适的用户发送虚构的消息，在这个阶段，每个参与者都要至少看30分个人资料，并向至少10个人发送消息，目的是有充分的的数据来评估他们的偏好，最后平均每个参与者查看了50.72分个人资料（s.d.=30.99），向11.92个用户发送了消息（s.d.=3.96），由于没有遵从指示，有3名参与者的数据被删除 选择解释方法我们返回最能解释为什么推荐的人是合适的k个属性来作为解释，为避免信息过载，限制k为3实验两个解释方法：1）Transparent；2）Correlation-based Transparent解释方法由RECON提供为了向x解释推荐y的原因，该算法返回y的k个属性，在用户x所有发过消息的用户中，用户y的这k个属性表现得最突出Transparent解释方法的算法如下图所示 correlation-based解释方法受机器学习中关联特征选择方法的启发产生估计某用户的特征值v_a和用户x会向该用户发送消息的可能性之间的关联对于用户x，定义I={i}为用户x查看过的用户集合，M_x(i)表示在用户集合I中x向哪些用户发送了消息，S_(x,v_a)(i)表示在用户集合I中哪些用户属性a的值为v_acorrelation-based解释方法的算法如下图所示 两个解释方法的区别使用example2.1的例子：Bob查看过25个人的资料，其中“从不吸烟”的人有18个，“身材苗条”的人有4个，而Bob向6个从不吸烟的用户，以及4个身材苗条的用户发送了消息。假设k=1，Transparent算法会将“从不吸烟”作为解释而不是“身材苗条”，因为Bob向“从不吸烟”的用户发送的消息更多；Bob向1/3的“从未吸烟”的人发了消息，向全部“身材苗条”的人发了消息，correlation-based算法会将“身材苗条”作为解释 比较两种解释方法的性能实验过程：使用4.1中的MM系统，让118名参与者中的59名再次进入系统，他们每个人会接收到由RECON算法生成的推荐列表，其中包含5名推荐用户，在59个参与者中，30个人会收到transparent的解释，29个人会收到correlation-based的解释，我们要求参与者分别对5个推荐进行评估，满分为5分，此外参与者还要填写一份调查问卷（问卷内容在第7部分中），调查问卷主要包括：用户对推荐的满意度、系统的可用性、系统的可理解性、对系统的信任程度、对推荐的解释是否有用，所有的问题满分都是5分实验中我们使用RECON推荐算法，因为双向协同过滤只能推荐发过消息的用户，本实验使用的系统不满足这个条件实验结果：使用correlation-based算法的解释比使用transparent算法的解释，在推荐满意度（mean= 3.58, s.d.= 0.82 vs. mean= 3.14, s.d.= 0.65,p ≤ 0.02）、系统可理解性（mean= 3.97,s.d.= 0.93 vs. mean= 3.41, s.d.= 0.65, p ≤ 0.04）、解释的有用程度（mean= 3.8, s.d.= 0.81 vs. mean=3.17, s.d.= 0.8, p ≤ 0.02）方面表现得更好，而在对推荐的评分、对系统的信任度、系统可用性方法没有明显区别基于上述实验结果，我们采用correlation-based算法进行接下来的实验 在虚拟在线约会系统中的评估已经有研究表明，不同的代价（cost），尤其是对被拒绝的担忧，对用户在系统中的行为有很大的影响，由于代价和潜在的利益在不同用户间是不同的，因此我们考虑两种模型：1）考虑明确代价的模型：用户来评估推荐结果而不考虑任何明确的代价和利益，类似在4.2中的实验2）考虑代价和利益 可忽略的代价实验过程：我们让剩余的59个参与者参与这个实验，每个参与者被随机分到如下两种情况的一种：1）使用one-sided解释（30人）2）使用互惠解释（29人）。每个用户再次进入系统中，接收到包含5个推荐用户的推荐列表，以及使用相应算法得到的解释，对推荐的衡量标准与4.2中类似实验结果：和期望的相反，几乎在所有的衡量标准上，ons-sided解释方法都优于互惠解释方法，如下图所示由于样本大小相对较小，因此很难评估子群体之间和参与者之间的差异，例如女性群体，而在接下来的实验中我们有较大的样本数据集，我们可以对子群体进行统计分析 明确的代价实验过程：我们有招募了67名未参与过实验的志愿者，年龄在18到35岁（average= 24.8 s.d.=4.74），参与者会被随机分到one-sided解释或互惠解释，和之前的实验类似，参与者进入到系统中生成个人资料、浏览他人的资料 、向他们觉得匹配的用户发送消息，但在推荐阶段，我们人为的设置了代价和利益：当接收到推荐时，参与者可以选择是否向推荐用户发送消息。如果没有发送，参与者不会加分或者减分；如果发送了消息，假设对方给与了回复，参与者会得到和两人之间匹配度相同的分数（由RECON算法评估），假设对方未给与回复，参与者会丢掉3分。这么做的 目的是鼓励参与者向感兴趣的用户发送消息，同时考虑被拒绝的可能。每个参与者会接收到包含5个推荐用户的推荐列表，以及使用相应算法得到的解释。定义接受度（acceptance rate）为参与者选择发送消息的推荐用户数量，之后参与者同样要填写调查问卷。实验结果：和前一个实验结果不同，这次的实验结果显示，互惠解释方法要优于one-sided解释方法，如下图所示（这里的图可能有错误） 在真实在线约会应用中的评估Doovdevan应用：为Android和IOS操作系统定制的web和移动应用程序，目前拥有大约32000名用户，并且正在迅速增长。我们选择他的原因是，在实验之前没有用户收到过推荐，这很重要，因为之前的推荐会影响用户对系统的信任度，以及用户对新的推荐的态度这次我们使用双向协同过滤推荐算法实验过程：随机选取161个活跃用户（至少一周登陆一次），其中包括78个男性和83个女性，年龄在18岁到69岁之间（mean= 36.1, s.d.= 3.01），每个用户会被随机分到one-sided解释或互惠解释，出于对隐私的担忧，我们不允许向推荐接收者透露推荐用户的偏好，因此互惠解释包括两个方面内容：1）推荐接收用户对推荐用户的预期兴趣的解释，包括推荐用户的特定属性；2）系统认为推荐接收用户符合推荐用户的偏好，因此他/她可能会做出肯定的答复每个用户同样接收到5个推荐用户，但不同的是用户每天会接到一个推荐，在这里，我们将接受度（acceptance rate）定义为用户发送了消息的推荐用户数除以用户点击了的推荐用户实验结果：实验结果显示接收到互惠解释的用户比接收到one-sided解释的用户的接受度更高（p&lt;0.05），接收到互惠解释的用户接受度为53%，而接收到one-sided解释的用户接受度为36%。此外，我们发现相比男性群体，互惠解释在女性群体中表现的更突出；相比于经常发送消息的用户，互惠解释在常发消息的用户中表现的更突出，如下图所示 我们还统计了在推荐之后的一周内用户的登录次数，作为解释方法的另一个潜在影响，我们发现接收到互惠解释的用户明显登录得更频繁（56 log-ins compared to 23 log-ins，p&lt;=0.05），这样的结果可能表明，收到互惠解释的用户对该系统更满意 讨论上述的实验结果表明，解释方法的选择取决于用户遵循建议的代价（cost），在高代价的环境中，选择互惠解释方法更好，我们推测这是因为在互惠解释中，附加信息使得用户对接受推荐的结果感到更有信心；而在代价可忽略的环境中，one-sided解释则优于互惠解释，我们推测有两个原因：1）信息过载：互惠解释包含更多的信息，如果推荐接收方认为这些信息不相关，则可能导致整个建议的有效性降低2）用户通常以与其他人不同的方式来评判自己的喜好，用户对吸引自己的原因可能与生成的解释不同我们进一步发现，并不是所有的用户都以同样的方式对解释作出反应，这可能意味着不太可能找到适用于所有人的解释方法，像遵循建议的代价就可能因人而异，比如男性更关注自己的喜好，而女性更关注自己对对方的吸引力我们的主要贡献是引入了互惠解释并评估了它的有效性需要注意的是，由于我们关注的是在线约会，上述结果并不能立即推广到其他互惠环境，例如招聘工作或室友匹配，互惠环境可能在其固有的担心被拒绝的情感成本上有很大的差异，这可能会影响互惠解释的有效性 总结我们介绍了相互解释的用法，其中包括对建议双方在匹配中的假定利益的推理我们对所提出的方法进行了评价，并将其与传统的片面解释方法进行了比较，并发现解释方法的选择应取决于用户接受建议的代价（例如情感代价），在高代价的环境中，选择互惠解释，而在代价可忽略的环境中，使用one-sided解释 附录]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>RecSys2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（十）-支持向量机]]></title>
    <url>%2F2019%2F10%2F08%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[本节课内容：1、从逻辑回归引出支持向量机的原理2、从直观上理解大间隔，及其数学原理3、SVM核函数的定义、使用、含义、选择4、探究逻辑回归和SVM算法的使用情景 优化目标我们从逻辑回归开始，看看如何做一些小小的改动来得到一个支持向量机，在逻辑回归中，我们已经熟悉假设函数以及sigmoid激活函数，如果我们有一个样本，其中y=1，我们希望h(x)尽可能接近1，因为我们想尽可能正确的将样本分类，如果h(x)≈1，就意味着z远大于0， 同样，对于另一个y=0的样本，我们希望h(x)趋近于0，就意味着z远小于0如果观察逻辑回归的代价函数，会发现每个样本（x,y）都会为总的代价函数增加这样一项，如下图蓝色箭头所示，如果将整个假设函数的定义带入其中，得到的就是每个样本对总体函数的具体贡献现在考虑两种情况，一个是y=1的情况，一个是y=0的情况。在y=1的情况中，代价函数只有第一项起作用，如果绘制得到的代价函数与z的关系，会得到如图一的一条曲线，当z很大时，函数所对应的值会变得很小，也就是说它对代价函数的影响很小，这也就解释了为什么逻辑回归在看见y=1的样本时会将z设成一个很大的值，因为它在代价函数中的对应这一项的值很小我们取z=1的点，绘制出新的代价函数曲线，在z=1点的右侧都是平的，代价函数值为0，左侧是与逻辑回归的代价函数曲线的趋势相似的一条直线，如图中粉色线，这就是当y=1时，我们要用到的代价函数cost_1，不难想象它和逻辑回归的效果很相似，事实上这会使支持向量机拥有计算上的优势，并使之后的优化问题变得简单，更容易解决以同样的方法可以得到y=0时的新的代价函数曲线cost_0，如下图中的图二所示对于支持向量机而言，我们实际要做的是将代价函数J(θ)中相应的位置替换为cost_1(z)和cost_0(z)，如下图所示支持向量机中的一些东西的写法会略有不同，比如代价函数的参数会稍微有些不同，首先我们要去掉1/m这一项，因为在使用支持向量机和逻辑回归时，人们遵循的惯例略有不同，同样能得到θ的最优解在逻辑回归中，我们的目标函数有两项，首先是训练样本的代价函数，其次是正则化项，要平衡这两项（A+λB），我们要做的是通过设定不同的正则化参数λ，以便能够权衡在多大程度上去适应训练集。对于SVM（支持向量机）而言，按照惯例我们会用一个不同的参数c，最小化cA+B，这只是一种不同的控制权衡的方式，或者说是一种不同的参数设置方法最终我们得到了支持向量机的总体优化目标，如图中粉色箭头所示，当最小化这个函数时，就得到了SVM学习得到的参数最后与逻辑回归不同的是，支持向量机并不会输出概率，相对的，我们得到的是通过优化这个代价函数得到一个参数θ，而支持向量机所做的是直接的预测，预测y=0或y=1，如果z≥0，那么假设函数就会输出1，反之会输出0 直观上对大间隔的理解下图显示了支持向量机的代价函数，左侧的图为关于z的代价函数cost_1(z)的曲线，用于正样本，而右边是关于z的代价函数cost_0(z)的曲线，由于负样本。想象一些如何使这些代价函数变的更小，如果你有一个正样本，那么仅当z≥1时，有cost_1(z)=0；反之，若y=0，只有当z≤-1时，有cost_0(z)=0这就相当于在支持向量机中构建了一个安全间距接下来考虑一种情况，我们常把常数c设成一个非常大的值，当最小化优化目标的时候，我们将迫切希望能找到一个值使得第一项等于0，要怎么做才能使第一项等于0？当有一个y=1的训练样本，如果要使第一项为0，那么要找到一个θ值使得θ^T x^((i) )≥1，同样的当有一个y=0的训练样本，如果要使第一项为0，那么要找到一个θ值使得θ^T x^((i) )≤−1。因此如果把优化问题看作是通过选择参数来使第一项等于0，那么优化问题就变成了下面这样，如图中蓝色字所示也就是说当解决这个优化问题的时候，会得到一个很有趣的决策边界观察下图的数据集，其中有正样本和负样本，这些数据线性可分，SVM会选择图中黑色的直线作为边界，显然它比粉色和绿色的直线要好得多，能更好的分开正样本和负样本，从数学的角度说，这条黑色的边界拥有更大的距离，也就是，黑色的线和训练样本的最小距离要更大一些，这个距离被称为支持向量机的间距，这使支持向量机具有鲁棒性，因为它在分离数据时会尽量用最大的间距去分离，因此支持向量机有时被称为大间距分类器现在的大间距分类器是在常数c被设的非常大的情况下得出的，给定这样一个数据集，也许我们会选择如图中黑色直线这样的一条决策边界，用大间距来分开正样本和负样本，现在SVM实际上要比这个大间距的视图更加复杂，尤其是它对异常点会很敏感，我们加进去一个额外的正样本，为了将样本用大间距分开，最后学习得到的可能是如图中粉色线这样的一条决策边界，着这样一个异常点的影响下，只因为这一个样本就将决策边界从黑色的线变为了红色的线，这是c被设的非常大的情况，但如果c比较小，最后得到的还会是这条黑线，甚至当数据不是线性可分的时，SVM依然能够正确的分类 大间隔分类器的数学原理（选修）首先将问题简化，我们假设θ_0=0，n=2根据向量内积的性质，可以得出θ^T x^(i)=p^(i) ||θ||，其中，p^(i) 表示x^(i) 向θ的投影的长度（如果是负数，则表示两个向量间夹角大于90°），||θ||表示向量θ的长度如下图所示，绿色直线为决策边界，由于θ_0=0，所以该直线是过原点的，θ所表示的向量与决策边界正交，即图中蓝色箭头我们取任意样本向θ做投影，其中红色线段表示正样本的投影，粉色线段表示负样本的投影如果是左图的决策边界，那么距离决策边界最近的正样本投影值会很小，为了让p^(i) ||θ||≥1，需要较大的||θ||，对于负样本也是如此；如果是右图的决策边界，尽量用较大的间隔去分离，只需要较小的||θ||，这就是SVM在做的事情 核函数给出如下图所示的一个训练集，我们要拟合出一个非线性的决策边界，来区分正负样例，一种办法是构造复杂多项式特征的集合，得到一个如下图所示的这样的多项式，这里我们移入一个新的符号f，f_i 表示第i个特征，比如f_1=x_1，f_2=x_2，f_3=x_1 x_2 等，如果原始数据集中就有很多特征，那么得到的多项式特征将会非常非常多，这对计算来说是非常大的压力，是否还有更好的方法来选择特征呢？ 构造新特征的方法这里我们只构造三个新特征，对于特征x_1 、x_2，我们手动选取一些不同的标记点，比如l^(1)、l^(2)、l^(3)，然后将新特征f_1 、f_2 、f_3 定义为一种相似度的度量，如图中蓝色字所示，即样本x与手动选取的点l^(i)的相似度similarity(x,l^(i))，这里的相似度就是核函数，这里使用的核函数实际上是高斯核函数，也可以使用不同的相似度核函数，核函数经常写作k(x,l^(i)) 核函数到底做了什么下图是刚刚用到的核函数，核心部分的分子是样本点x与标记点l^(i)的欧氏距离的平方，这里我们没有考虑x_0，x_0 总是等于1假设x与其中一个标记点非常接近，那么两点间的欧式距离约等于0，核函数的结果就约等于1；假设x与其中一个标记点非常远，那么两点间的欧氏距离就会非常大，核函数的结果会接近于0每一个标记点会定义一个新的特征，也就是说给定一个训练样本x，基于标记点，我们可以计算得到新的特征我们将核函数画出来，如下图 所示，在三维图形中的高度表示核函数的结果，样本点位于下方的平面上，假设选取的标记点为（3,5），那么越接近这个标记点核函数的结果越接近于1，越远离这个标记点，核函数的结果越接近于0此外σ是高斯核函数的参数，从图中我们也可以看出，当σ较大时，从标记点移走时，特征变量的值减小的速度较慢 得到的预测函数假设我们已经得到了θ参数的值，如下如图所示，我们就会得到一个预测函数给定一个样本x，如图中粉色点，我们希望计算出它的y值，由于这个样本点很接近l^((1) )，又远离l^((2) ) 和l^((3) )，所以f_1 约等于1，f_2 、f_3 约等于0，最后多项式的结果大于等于0，预测y=1再给定一个样本x，如图中浅蓝色点，该样本点远离l^(1)、l^(2)、l^(3)，所以f_1 、f_2 、f_3 都约等于0，最后多项式的结果小于0，预测y=0我们发现，对于接近于l^(1)、l^(2)的点，会预测y=1，对于远离l^(1)、l^(2)的点，会预测y=0，所以会得到红色线这样的决策边界，线内部预测y=1，线外部预测y=0综上，我们可以由核函数和标记点来训练出非常复杂的非线性决策边界 如何获得标记点？在实际的机器学习问题中，我们会不仅仅选取3个标记点，我们只需要直接将样本点作为标记点，使用这个方法，我们最终会得到m个标记点，即每一个标记点的位置都与每一个样本点的位置相对应，这说明特征函数基本上是在描述每一个样本距离我们来具体描述一下这个过程：1、给定一个包含m个数据的训练集，将每个样本点都选做标记点，得到m个标记点2、给定一个样本x^(i)，这个样本可以来自于训练集、测试集、验证集，计算该样本点与m个标记点的相似度，作为该样本点新的特征向量f^(i)，其中f_0^(i)=1，由于l^(i)=x^(i)，因此f_i^(i)=13、使用新的特征向量f^(i)作为样本进行训练在实际使用支持向量机进行训练时，最后一项可能会稍作变化，使其可以应用到更大的数据集上，并得到更高的效率 如何选择支持向量机中的参数？支持向量机中，参数c的作用与λ类似，λ是逻辑回归中的正则化参数。当c较大时，对应着逻辑回归中λ较小的情况，这意味着不使用正则化，可能得到一个低偏差高方差的模型，更倾向于过拟合；当c较小时，则可能得到高偏差低方差的模型，更倾向于欠拟合另一个要选择的参数是高斯核函数中的σ。当σ偏大时，相对应的相似度函数倾向于平滑，变化比较平缓，这会给模型带来较高的偏差和较低的方差；反之当σ较小时，相似度函数会变化的很剧烈，会给模型带来较低的偏差和较高的方差 使用SVM我们不需要自己动手实现SVM的算法，但需要选取参数和核函数一种常用的是不使用核函数，也就是线性SVM，如果有大量的特征，但训练的样本数很少，可能只想拟合一个线性的决策边界，而不是特别复杂的多项式决策边界，因为训练样本太少，可能会过拟合另一种常用的核函数是高斯核函数，使用高斯核函数还需要选取参数σ，如果我们的训练集的特征较少，而训练样本数量很多，可能会想拟合一个比较复杂的非线性决策边界，可以选择高斯核函数。此外，如果选用高斯核函数，必要的情况下需要对原始特征变量进行缩放不论使用什么核函数都需要满足一个技术条件，叫做默塞尔定理，这个定理所做的就是确保所有的SVM软件包都能用大类的优化方法从而迅速得到参数θ还有另外一些不常用的核函数，比如多项式核函数以及一些更复杂的核函数，多项式核函数通常都要求x和l都是严格非负的，这样可以保证x和l的内积永远不会是负数，从而当x和l很接近时，内积也会很大 逻辑回归与SVM的选择假设n代表训练集的特征数量，m代表训练集的样本数量如果n很大，n=10000+，m较小，m∈(10,1000)，使用逻辑回归或者线性SVM如果n较小，n∈(1, 1000），m大小适中，m∈(10,10000)，使用高斯核函数的SVM如果n较小，n∈(1, 1000），m很大，m=50000+，这时高斯核函数将会计算的很慢，可以手动创建更多的特征变量，然后使用逻辑回归或者线性SVM逻辑回归和线性SVM是很相似的算法，在其中一个算法应用的问题中，另一个算法可能也很有效最后对于所有的这些问题，设计不同的神经网络可能会非常有效，但有时不会使用神经网络的原因是，对于许多问题，神经网络训练起来可能会特别慢。此外，SVM具有的优化问题，是一种凸优化问题，SVM包总会找到全局最小值或者接近它的值，因此对于SVM不用担心局部最优，但实际在神经网络中，局部最优是一个不大不小的问题 课程资料课程原版PPT-Support Vector Machines练习内容和相关说明相关数据ex3data1 python代码实现1-linear-SVM.ipynb2-Gaussian-kernels.ipynb3-search-for-the-best-parameters.ipynb4-spam-filter.ipynbML-Exercise6.ipynb]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Angular：判断ng-repeat循环结束，监听DOM渲染完成]]></title>
    <url>%2F2019%2F09%2F27%2FAngular%EF%BC%9A%E5%88%A4%E6%96%ADng-repeat%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9D%9F%EF%BC%8C%E7%9B%91%E5%90%ACDOM%E6%B8%B2%E6%9F%93%E5%AE%8C%E6%88%90%2F</url>
    <content type="text"><![CDATA[方法一：自定义指令，监听ng-repeat状态本方法只适用于ng-repeat遍历的数据内容不变的情况如果数据内容改变，使用此方法会出bug：假设列表A包含3项内容，列表B中包含4项内容，当数据从A列表切换到B列表时，会触发link方法，但当数据从B列表切换到A列表时，则不会触发。也就是数据列表中的项由多变少时不会触发link事件 html部分代码： 123&lt;ul&gt; &lt;li ng-repeat=&quot;item in data track by $index&quot; repeat-finish&gt;&lt;/li&gt;&lt;/ul&gt; js部分代码： 12345678910angular.module(&quot;mainApp&quot;).directive(&apos;repeatFinish&apos;, [function () &#123; return &#123; restrict: &apos;A&apos;, link: function (scope, element, attr) &#123; if(scope.$last === true) &#123; // ng-repeat循环完成后要做的事情 &#125;; &#125;, &#125;;&#125;]); 方法二：使用$timeout本方法可以用于ng-repeat遍历的数据内容改变的情况$timeout是angular为了能自动触发脏检测而封装的方法，是异步的将你需要执行的方法放在$timeout中，它就会等到所有的dom渲染完成以及同步逻辑跑完后执行 html部分代码： 123&lt;ul&gt; &lt;li ng-repeat=&quot;item in data track by $index&quot;&gt;&lt;/li&gt;&lt;/ul&gt; js部分代码： 123$timeout(function () &#123; // 处理dom加载完成，或者ng-repeat循环完成后要做的事情&#125;,0);]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>Angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（九）-机器学习系统设计]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[本节课内容：1、解决机器学习问题的大致步骤2、使用误差分析来确定优化学习算法的方向3、介绍查准率、召回率和F值的概念、定义及其意义4、机器学习问题在什么情况下可以通过大量数据来提升算法性能 确定执行的优先级以垃圾邮件分类为例描述解决问题的过程为了应用监督学习，首先想到的是如何来表示邮件的特征向量x，通过特征向量x和分类标签y，我们就可以训练一个分类器，比如使用逻辑回归的方法，这里有一种选择邮件特征向量的方法我们提出一个可能含有100个单词的列表，通过这些单词来区分垃圾邮件或非垃圾邮件，假如有一封邮件，我们可以将这封邮件的各个片段编码成如下图的一个特征向量，先将找出的100个单词按照字典序排序（也可以不排序），当邮件中包含某个单词时，该单词出现在特征向量中的位置上置1，否则置0，最后我们可以得到100维的特征向量实际工作中的普遍做法是挑选出出现频率最多的n个单词，n一般在10000到50000之间如何在有限的时间内，让你的垃圾邮件分类器具有高精准度和低错误率？1、收集大量数据2、用更复杂的特征描述邮件，比如将发件人的信息包括其中3、关注邮件的主体部分，构建更复杂的特征，比如单词原型和其变型是否要看做同一个单词、字母的大小写、标点符号等4、设计更复杂的检测邮件中出现的故意拼写错误 误差分析如果你 准备从事研发机器学习产品，或者开发机器学习应用，通常来说最好的方法不是建立一个很复杂的，有许多复杂特征的系统，而是通过一个简单的算法来快速实现它，然后通过交叉验证来测试数据，之后就能绘制相应的学习曲线，通过绘制学习曲线预计检验误差，来找出你的算法是否存在高方差或者高偏差问题，或者一些其他问题，在做出这些分析后，来决定是否使用更多的数据或者特征等等这种方法在你刚刚开始解决机器学习问题的时候，能起到很好的作用，因为你不能预知是需要更多的特征，还是需要更多的数据，或者别的东西这种思想告诉我们，我们应当用实际的证据来指导我们的决策，来决定把时间花在哪，而不是凭直觉除了绘制学习曲线，误差分析也是很有意义的事，比如当实现垃圾邮件分类器的时候，我们经常会观察交叉验证集的情况，然后看一看那些被算法错误分类的邮件，看看这些经常被错误分类的邮件有什么共同的特征和规律，这个过程就会启发你设计怎样的新特征，或者告诉你现在的系统有什么优点和缺点，然后指导你想出办法解决 以垃圾邮件分类分类器为例假设你在做一个垃圾邮件分类器，然后在你的交叉验证集中有500个样本，假设该例子中算法有很高的错误率，它错误地分类了100个交叉验证样本现在要做的就是手动核查这100个错误，然后手工为他们分类，同时要考虑这些邮件是什么类型的邮件，有什么线索或特征能够帮助算法正确分类。假设数出了不同类别的邮件数量如下图所示，会发现这个算法在判断盗号邮件（steal password）时总是表现的很差，这说明你应该花更多的时间仔细研究这类邮件，看看是否可以想到更好的特征来为他们正确分类与此同时，还应该做的是看看有什么线索或额外的特征，能帮助算法识别这种邮件。假设能够帮我们更好的进行分类的方法或特征，是检查故意拼写错误和奇怪的邮件来源，以及垃圾邮件特有的标点方式，与之前一样，手工检查这些邮件并统计各类型数量，发现很多垃圾邮件都以奇怪的标点方式 ，这就是一个很明显的信号，说明这值得你花时间去构造更复杂的基于标点符号的特征因此这样的误差分析，是一种手动的检查算法所出现的失误的过程，它能引导你走向最有成效的道路我们真正要做的是找出这种算法最难以分类的样本，对于不同的学习算法而言，对他们造成困难的样本总是相似的，通过一个简单粗暴的算法实现，你可以很快找到算法的不足所在，和难以处理的样本类型，然后把精力集中在他们身上 改进学习算法在改进算法时的一个技巧是，保证自己对学习算法有一种数值估计的方法，也就是当你改进学习算法时，如果你的算法能够返回一个数值评价指标来估计算法的执行效果，将会很有帮助，这个数字能够告诉你，你的学习算法效果有多好假如我们正在决定是否应该将单词的原型和变型看做同一个单词，这样做的一种方法是只看一个单词中的前几个字母，在自然语言处理中，这种方法是通过一种词干提取软件来实现的，但这种词干提取软件只会简单的检查单词的前几个字母，这可能有坏处，比如可能会把universe和university看成一个单词。因此，当你决定要不要使用这种方法来进行垃圾邮件分类，就很难做出抉择，特别是，即使误差分析也可能无法帮助你决定提取词干到底是不是一个好主意，此时最好的方法就是用最快的方式尝试使用一下这个方法，然后看它是否能起到效果，通过数值方法来评估算法的效果将会非常有用。最容易想到的方法就是通过交叉验证，在使用和不使用词干提取时，各自的错误率来估计算法的效果对于这个特定的问题这个单一规则的数值评价指标就叫做交叉验证错误率，在之后，这种带有数值评价指标的例子还需要进行一些其他的处理另一个是否要区分大小写的例子与之类似因此当你要改进你的学习算法时，你总要尝试很多新主意和新版本的算法，如果每次你使用新方法，都手动的去检测这些例子，看看他们表现的好不好，会很难让你做出决定，但通过单一规则的数值评价指标，你可以观察误差率是变大还是变小了，你可以用它更快的实践你的新想法，它能直接告诉你，你的想法能提高还是降低学习算法的表现，大大加速进程此外，强烈推荐在交叉验证集上做误差分析，而不是测试集，如果在测试集上做误差分析，从数学的角度来讲是不合适的 不对称性分类的误差评估前面的课程中，我们提到了误差分析，以及设定误差度量值的重要性，有了算法的评估和误差度量值，有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的机器学习算法造成非常微妙的影响，这就是偏斜类问题以之前的癌症分类为例，我们拥有内科病人的特征变量，我们想知道他们是否患有癌症，这就像恶性与良性肿瘤的分类问题，我们训练逻辑回归模型，假设用测试集检验了这个分类模型，并且发现只有1%的 错误率，看起来是非常不错的效果，但是假如我们发现在测试集中只有0.5%的患者真正得了癌症，因此在我们的筛选程序里，只有0.5%的患者得了癌症，因此1%的错误率就不再显得这么好了举个具体例子，图中代码忽略了输入值x，它总y总等于0，于是它总是预测没有人得癌症，那么这个算法实际上只有0.5%的错误率，这甚至比我们之前得到的1%的错误率还要好这种情况发生在正例和负例的比率非常接近于一个极端情况，在这个例子中，正样本的数量与负样本的数量相比非常非常少，我们把这种情况叫做偏斜类在这种情况下，使用分类误差或分类精确度来作为评估度量，可能会产生如下问题：假设有一个算法它的精确度是99.2%，假设对这个算法做出了一点改动，现在得到了99.5%的精确度，这到底是不是算法的一个提升呢？用某个实数作为评估度量值的一个好处是，它 可以帮我们迅速决定我们是否需要对算法做出一些改进，将精确度从99.2%提升到99.5%，但我们的改进到底是有用的还是说我们只是把代码替换成例如总是预测y=0这样的东西，因此如果有一个偏斜类，用分类精确度并不能很好的衡量算法 当我们遇到偏斜类时，我们希望有一个不同的误差度量值，或者不同的评估度量值，其中一种评估度量值叫做查准率（Precision）和召回率（Recall） 假设我们正在用测试集来评估一个分类模型，对于测试集中的样本，每个测试样本都会等于0或1（假设这是一个二分问题），我们的学习 算法要做的是做出值的预测，且算法会对测试集中的每个实例做出预测，预测值也是0或1，基于这些值我们绘制一个2X2的表格，如下图所示如果实际值为1预测值也是1，那么我们把这个样本叫做真阳性（True positive）；如果实际值为0预测值也为0，那么这个称为真阴性（True negative）；如果预测值为1但实际值为0，将其称为假阳性（False positive）；如果预测值为0但实际值为1，则将其称为假阴性（False negative） 接下来计算查准率和召回率：查准率（Precision）：对于所有我们的预测，患有癌症的病人，有多大比率的病人是真正患有癌症的，即真阳性的样本数，除以所有预测为阳性（值为1）的样本数。查准率越高越好召回率（Recall）：对于测试集或者交叉验证集中确实得了癌症的病人，有多大比率我们正确预测了他们得了癌症，即真阳性的样本数量，除以实际为阳性的样本数量。召回率越高越好通过计算查准率和召回率，我们能更好的知道分类模型到底好不好。具体来说，如果我们有一个算法总是预测y=0，即总是预测没有人得癌症，那么这个分类模型的召回率为0最后需要记住，在查准率和召回率的定义中，我们总是习惯性的用y=1表示出现的比较少的类 查准率和召回率的权衡查准率和召回率作为遇到偏斜类问题的评估度量值，在很多应用中，我们希望能够保证查准率和召回率的相对平衡我们继续用癌症分类的例子，假设用逻辑回归模型训练了数据，输出在0-1之间的概率值，我们可以根据结果计算查准率和召回率 情况一：假设只有在我们非常确信的情况下，才预测一个病人得了癌症修改算法，不再将临界值设为0.5，可能设为0.7（或更高），这样他有大于等于70%得癌症的概率，如果这样做，那么你的回归模型会有较高的查准率，因为所有预测得了癌症的病人，又比较高的可能性他们真的得了癌症，与之相反，这个回归模型会有较低的召回率。如果将临界值再增大，查准率会继续提高，而召回率会继续降低 情况二：假设我们希望避免漏掉患有癌症的病人这种情况下，我们不再设置高的临界值，而是将临界值降低，比如0.3，这样，我们会得到一个较高召回率的模型，因为确实患有癌症的人有很大一部分被我们正确预测出来，但我们会得到较低的查准率，因为我们预测患有癌症的病人比例越大，就有较大比例的人其实没有患癌症而被预测为患有癌症 对于大多数回归模型，要权衡查准率和召回率，当你改变临界值的时候，可以画出曲线来权衡查准率和召回率，此外查准率和召回率的曲线可以是各种不同的形状，这取决于回归模型的具体算法 有没有办法自动选取临界值？更广泛的说，如果我们有不同的算法，我们如何比较不同的查准率和召回率？假设我们有三个不同的学习算法，或者这三个不同的学习曲线是相同的模型但临界值不同，我们怎样决定哪一个算法是最好的？ 第一种方法是计算一下查准率和召回率的平均值，看看哪个模型有较高的均值，但这可能并不是一个很好的解决办法。因为像我们之前的例子一样，如果我们的回归模型总是预测y=1，可能得到非常高的召回率，而非常低的查准率；相反的，如果我们的模型总是预测y=0，会得到非常高的查准率，而非常低的召回率。他们中的任何一个都不是好模型，我们可以通过非常低的查准率或非常低的召回率，来判断这不是一个好模型，但如果只使用均值，图中算法3是最高的，即使可以通过使用总是预测y=1这样的方法也可以得到，但这并不是一个好模型，因此均值不是评估算法的好方法 另一种结合查准率和召回率的不同方式叫做F值，也叫做F1值，它会考虑一部分查准率和召回率的均值，但它会给查准率和召回率中较低的值更高的权重，F值的分子是查准率和召回率的乘积，因此如果查准率或召回率等于0，F值也会等于0，可以看出F值结合了查准率和召回率，但如果F值较大，那么查准率和召回率都必须较大 机器学习数据考虑一个问题，如何在易混淆的词之间进行分类，如下图所示的例子，把诸如这样的机器学习问题当做一类监督学习问题并尝试将其分类，什么样的词在一个英文句子特定的位置才是最合适的，假设选取如下四种分类算法，改变训练集的大小，并将这些学习算法用于不同大小的训练数据集中，右侧的折线图就是得到的结果，趋势非常明显，随着数据集的增大，这些算法的性能也都对应的增强了已经有一系列不同的研究显示了类似的结果，这些结果表明，许多不同的学习算法有时倾向于依赖一些细节，并表现出相当相似的性能，但真正能提高性能的是你能够给予一个算法大量的训练数据，像这样的结果引起了在机器学习中的常用说法“并不是拥有最好的算法的人能成功，而是拥有最多数据的人”那么这种情况什么时候是真，什么时候是假呢？假如有这样一些假设，在这些假设下有大量我们人为有用的训练集，我们假设在我们的机器学习问题中，特征x包含了足够的信息，这些信息可以帮助我们用来准确的预测y举一个反例，设想一个预测房屋价格的问题，房子只有大小信息，没有其他特征，如房子的地理位置、房间数量、装修情况、房子新旧等其他信息，然而这些都会影响房子的价格，不仅仅是房子的大小，因此如果只知道房子的大小，是很难预测它的价格的，这个例子中特征x就没有包含足够的信息假设特征x包含足够的信息用来预测y值，假设使用需要大量参数的学习算法，也许是有很多特征值的逻辑回归或线性回归，或有许多隐藏单元的神经网络，这些都是强大的学习算法，它们有很多参数，这些参数可以拟合非常复杂的函数，因此将这些算法想象成低偏差算法，如果我们在训练集上运行这些算法，它将很好的拟合训练集，训练误差就会很低现在假设我们使用了非常非常大的训练集，这种情况下，尽管我们希望有很多参数，但如果训练集比参数的数量更多，那么这些算法就不太可能会过拟合，也就是训练误差可能接近测试误差将上述两个结果结合起来，即训练误差很低，以及训练误差接近于测试误差，可以得到测试误差也会很小 另一种考虑这个问题的方式是，为了有一个高性能的学习算法，我们希望它不要有高的偏差和方差，偏差问题我们通过确保有一个具有很多参数的学习算法来保证，并通过用非常大的训练集来保证没有高方差问题，将这两个条件放在一起，我们最终可以得到一个低偏差和低方差的学习算法 从根本上来说，特征值有足够的信息量是一个关键的假设 总结：如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式来得到一个高性能的学习算法，最关键的在于特征x给出了足够的信息量来预测y值 课程资料课程原版PPT-Machine learning system design]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（八）-应用机器学习的建议]]></title>
    <url>%2F2019%2F09%2F18%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[本节课内容：1、介绍改进学习算法的几种方法，介绍训练集、验证集和测试集2、高偏差与高方差问题的区分方法，及相应的解决方法3、绘制学习曲线帮助判断高偏差与高方差问题 机器学习诊断法：通过执行一种测试，能够了解算法在哪里出了问题，这通常能告诉我们，想要改进一种算法的效果，什么样的尝试才是有意义的 评估假设怎样判断一个假设是否过拟合？对于下图的简单例子，我们可以画出假设函数h(x)然后观察，但对于一般情况，特征不止一个，还有包含很多特征的问题，想要通过画出假设函数来观察就变得很难甚至不可能了，因此需要另一种评价假设函数的方法假设我们有一组训练样本，如下图所示，为了确保可以评价我们的假设函数，要将这些数据分为两部分。第一部分将成为训练集（training set），第二部分将成为测试集（test set），其中一种典型的分割方式是按照7:3的比例，70%的数据作为训练集，30%的数据作为测试集一般来说，下标为test表示，这些样本来自测试集另外一点，这里我们选择前70%的数据作为训练集，后30%的数据作为测试集，但如果这组数据有某种规律或顺序的话，最好是随机选择 一种典型的方法训练和测试学习算法线性回归：首先，需要对训练集进行学习得到参数θ，即最小化训练误差J(θ)，这里使用训练集接下来，计算测验误差，使用J_test来表示测试误差，然后把从训练集中学习得到的参数θ，放入J_test(θ)中计算测试误差，这实际上是测试集误差平方的平均值这里是当我们使用线性回归和平方误差标准时，测试误差的定义分类问题，比如逻辑回归：训练和测试逻辑回归的步骤与之前所说的非常类似，首选使用训练集进行学习得到参数θ，再使用下图中J_test(θ)公式在测试集上计算测试误差另一种形式的测试度量，更易于理解，叫做错误分类，也被称为0/1分类错误，0/1表示了预测分类正确或错误的情况，然后应用错误分类误差来定义测试误差，实际上就是假设函数错误标记的测试集中的样本 模型选择和训练、验证、测试集在过拟合的情况中，学习算法即使对训练集拟合的很好，也并不代表它是一个好的假设，这也是为什么训练集的误差不能用来判断该假设对新样本的拟合好坏更普遍的规律是，如果参数对某个数据集拟合的很好，那么用同一数据集计算得到的误差，比如训练误差，并不能很好的估计出实际的泛化误差，即该假设对新样本的泛化能力 模型选择假设要选择能最好的拟合数据的多项式次数，这类似于在算法中加入一个参数d，表示多项式次数，所以除了参数θ，还有一个参数d，需要用数据集来确定1、首先选择一个模型，然后最小化训练误差，这样会得到一个参数向量θ^((d))，表示用次数为d的 多项式模型拟合数据得到的参数2、对所有这些模型求出测试集误差，即可以算出J_test(θ^((d) ))3、为了从这些模型中选出最好的一个，要看哪个模型有最小的测试误差，假设对这个例子最终选择了五次多项式的模型 泛化能力检测现在我们要使用最终的五次多项式模型，但它的泛化能力怎么样？我们可以观察这个五次多项式模型对测试集的拟合情况，但问题是，这样做仍然不能公平的评估出这个模型的泛化能力，因为我们用测试集拟合了额外的参数d，又在测试集上评估假设模型为解决在模型选择中出现的问题，我们通常会蚕蛹如下方法来评估假设：给定一个数据集，将它分为三个部分，第一部分叫做训练集（training set），第二部分称为交叉验证集（cross validation set，简记为cv），有时候也叫验证集，最后一部分叫做测试集（test set），这些数据的典型分配比例是60%作为训练集，20%作为交叉验证集，20%作为测试集，这些比值可以稍微调整一般来说，下标为cv表示，这些样本来自交叉验证集再回到模型选择问题，我们要做的就是用交叉验证集来选择模型，而不是原来的测试集，最后再用测试集衡量或估计算法选出的模型的泛化误差 诊断偏差与方差当运行一个学习算法，如果这个算法的表现不理想，那么多半是出现了两种情况，要么是偏差比较大，要么是方差比较大，换句话说，要么是欠拟合问题，要么是过拟合问题，这时候搞清楚是偏差问题还是方差问题或者两者都有关就很重要，因为弄清楚到底是哪方面的问题，就能很快找到有效的方法和途径来改进算法 欠拟合与过拟合下图中的三种情况分别表示欠拟合、刚好、过拟合，其中第二种刚好的情况的泛化误差也是三种情况中最小的我们沿用之前所使用的训练误差和验证误差的定义，分别是用训练集计算的和用交叉验证集计算的均方误差，接下来，画出训练误差和验证误差分别与多项式次数的关系的示意图，横坐标表示的是多项式的次数，纵坐标表示误差随着多项式次数的增加，模型对训练集拟合的越来越好，图中粉色线表示训练误差与多项式次数的关系再看验证误差，实际上如果观察测试误差的话，会得到和验证误差类似的结果：如果d=1，意味着用一个很简单的函数来拟合数据，也许不能很好的拟合训练集，会得到较大的验证误差；如果用中等大小次数的多项式来拟合时，会得到更小的验证误差，因为找到了能够更好的拟合数据的次数；如果次数d太大，那么我们又过拟合了数据，会得到较大的交叉验证误差。图中 红色线表示验证误差与多项式次数的关系 如何判断是高偏差问题还是高方差问题？交叉验证误差比较大的情况对应着曲线的最左侧和最右侧，那么左侧这一端对应的就是高偏差的问题，需要一个较高的多项式次数来拟合数据，相反的，右侧这一端对应的就是高方差的问题，需要一个较小的多项式次数来拟合数据这幅图也提示了我们如何区分这两种情况：具体地说，对于高偏差问题，也就是欠拟合现象，交叉验证误差和训练误差都会很大；对于高方差问题，也就是过拟合现象，训练误差会很小，而交叉验证误差会很大 正则化和偏差、方差正则化参数λ对模型的影响假设我们要对下图中的高阶多项式进行拟合，为了防止过拟合，要使用正则化，也就是试图通过正则化来让参数的值尽可能小，正则化项求和的范围照例是1到m，而非0到m，然后分析三种情况：第一种情况是正则化参数λ很大，这种情况下所有的θ参数都将接近于0，h(x)将会等于或近似等于θ_0，因此我们最终得到的假设函数大致如下图中的图一所示，近似于一条水平的直线，因此这个假设处于高偏差，对数据集严重欠拟合另一种极端情况是正则化参数λ很小，这种情况下要拟合一个高阶多项式的话，通常会出现过拟合的情况，也就是在拟合一个高阶多项式时，如果没有进行正则化，或者正则化程度很小时，通常会得到高方差、过拟合的结果，如下图中图三所示只有当取一个大小适中的正则化参数λ，才能得到一组对数据拟合比较合理的θ参数值，如下图中图二所示 如何自动的选择出一个最合适的正则化参数λ的值？假设在使用正则化的情形中，h_θ(x)为模型，J(θ)为学习算法的目标（代价函数），定义训练误差（J_train(θ)）为另一种不同的形式，即训练集的平均平方和误差，不考虑正则化项，与此类似，定义交叉验证集误差（J_cv(θ)）和测试误差（J_test(θ)），分别是交叉验证集和测试集上的平均平方和误差自动选择正则化参数λ的值：1、选取一系列想要尝试的λ值，以下图为例，选取12个不同的λ值（λ从0.01开始，以2倍速增长），得到12个不同的备选模型2、选用第i个模型，最小化代价函数J(θ)，得到某个参数向量θ，使用θ^((i)) 表示这个向量3、对于所有的θ参数，用交叉验证集来评价它们，选取12个模型中，交叉验证误差最小的模型作为最终选择4、观察最终选出的模型在测试集上的表现情况 J_train和J_cv与正则化参数λ的关系（分别对应训练集和交叉验证集）当λ很小时，也就是几乎没有使用正则化，有很大的可能处于过拟合的情况，也就是对训练集拟合相对较好，即J_train很小，而J_cv很大当λ很大时，很有可能处于高偏差、欠拟合的情况，也就是不能对训练集很好的拟合，即J_train很大，J_cv也很大因此，当λ增大时，J_train增大（图中蓝色线），J_cv先减小后增大（图中粉色线），图中左端为高方差，右端为高偏差。同样的，总会有中间的某个λ值，表现的刚好合适，此时的训练误差和交叉验证误差都较小 学习曲线绘制学习曲线为了绘制学习曲线，通常会先绘制出J_train或J_cv关于参数m的函数，m表示训练集样本总数，m通常是一个常数，但我们需要减小训练集，限制只用10、20、30或40个训练样本，然后对于这些小的训练集画出训练误差，以及交叉验证误差 假设只有1个训练样本，假设用二次函数来拟合，由于只有一个训练样本，拟合的结果很明显会很好，其误差一定为0假设有2个训练样本，二次函数也能很好的拟合，即使使用正则化，拟合的结果也会很好，如果不使用正则化，就能完美拟合假设有3个训练样本，二次函数依然能很好的拟合也就是说，当m=1，m=2，m=3时，如果不使用正则化，训练误差都会等于0；如果使用正则化，训练误差会稍大于0。因此，当训练样本容量m很小时，训练误差也会很小 再假设有4个训练样本，二次函数就不能对数据拟合的很好假设有5个训练样本，二次函数的拟合结果就一般般了当训练集越来越大的时候，要保证使用2次函数，对所有的样本拟合效果依然很好，就越来越困难因此，随着训练集容量m的增大，不难发现平均训练误差在增大，如果画出这条曲线，会发现训练误差随着m的增大而增大（图中蓝色线） 对于交叉验证误差，当训练集很小时，泛化程度不会很好，也就是不能很好的适应新样本，因此这个假设不是一个理想的假设，只有使用更大的训练集时，才能得到一个能够更好拟合新数据的假设，因此交叉验证误差和测试误差都会随着训练集样本容量m的增大而减小（图中粉色线），因为使用的数据越多，越能获得更好的泛化表现 高偏差与高方差下的学习曲线假设出现高偏差问题：为了更清楚的解释这个问题，我们使用一个简单的例子来说明，也就是用一条直线来拟合数据，很显然直线不能很好的拟合数据现在想象如果增大训练集样本容量会发生什么，不难发现最后还会得到类似的一条直线，也就是把样本容量扩大，这条直线也基本不会变化太大所以如果绘制交叉验证误差，会得到图中蓝色线，最左端表示，训练样本很小时，表现相当不好，而当训练样本容量增大到某个值的时候，就会找到那条最有可能拟合数据的那条直线，此时即使继续增大训练样本容量，还会得到一条差不多的直线，也就是交叉验证误差或训练误差会很快变为水平而不再变化如果绘制训练误差，会得到图中粉色线，训练误差一开始是很小的，而在高偏差的情况下，训练误差会逐渐增大，最后接近交叉验证误差，因为参数很少，而训练数据很多，当m很大时，训练误差和交叉验证误差会很接近高偏差的问题可以由很高的交叉验证误差和训练误差反映出来结论：如果一个学习算法有高偏差，随着我们增加训练样本，交叉验证误差不会明显下降了，所以如果学习算法处于高偏差的情形，那么选用更多的训练集数据，对于改善算法表现无益假设出现高方差的问题：假训练样本很少的情况，我们用很高次数的多项式来拟合，并假设使用很小的λ，最终会对这组数据拟合的很好，且这个函数会对数据过拟合，此时训练误差会很小随着训练样本容量的增加，可能仍然会有些过拟合，但此时要对数据很好的拟合变得更加困难了，所以随着训练样本容量的增大，训练误差也会随之增大，因为训练样本越多的时候，就越难把数据集拟合的很好，但总的来说训练集误差还是很小（图中蓝色线）对于交叉验证误差，在高方差的情形中，假设函数对数据过拟合，因此交叉验证误差将会一直很大（图中粉色线）高方差问题的一个明显的特点就是，在训练误差和交叉验证误差之间，有一段很大的距离总结：如果一个学习算法有高方差，如果我们考虑增加训练集的样本数，训练误差和交叉验证误差的学习曲线会相互靠近，因此可以预测，如果继续增大训练样本的数量，交叉验证误差将会一直下降，所以使用更多的训练数据，对改进算法是有帮助的因此当要改进一个学习算法时，通常会画出学习曲线，可以更好的判断偏差或方差问题 决定接下来做什么选择合适的方法改进算法假设我们试图用正则化的线性回归拟合模型，发现并没有达到我们预期的效果，我们提出了很多改进算法的选择（如下图所示），怎么判断哪些方法有效呢？1、获取更多的训练样本：对于解决高方差问题是有帮助的，但对解决高偏差问题不会有什么帮助2、减少特征数量：对高方差时有效3、选用更多的特征：通常是解决高偏差问题的一个方法4、增加多项式特征：实际上属于增加特征，是另一种修正高偏差的方法5、减小λ：修正高偏差6、增大λ：修正高方差 神经网络的模型选择当进行神经网络拟合时，可以选择相对比较简单的神经网络模型，比如只有很少甚至只有一个隐含层，并且只有少量的隐含单元，像这样简单的神经网络，参数不会很多，容易出现欠拟合的情况，这种小型神经网络的最大优势在于计算量较小另一种情况是拟合较大型的神经网络结构，比如每层隐藏层的单元数很多，或者有很多隐藏层，这种比较复杂的神经网络参数较多，容易出现过拟合的情况，这种结构的一大劣势（也许不主要，但还是要考虑），就是当有大量神经元时，会有很大的计算量，但通常不构成问题。这种大型神经网络最主要的潜在问题，还是它更容易出现过拟合现象，通常越大型的网络性能越好，如果发生过拟合，可以使用正则化的方法来修正，一般来说，使用一个大型神经网络并用正则化来修正过拟合问题，通常比使用一个小型的神经网络效果更好。最后还要选择隐藏层的层数，通常来说，使用一个隐藏层是比较合理的默认选项，但如果想要选择最合适的隐藏层层数，也可以试试把数据集分成训练集、验证集和测试集，然后训练一个隐藏层的网络，然后试试两个、三个隐藏层，然后看看哪个神经网络在交叉验证集上表现的最理想 课程资料课程原版PPT-Advice for applying machine learning练习内容和相关说明相关数据ex5data1 python代码实现ML-Exercise5.ipynb]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（七）-神经网络参数的反向传播算法]]></title>
    <url>%2F2019%2F09%2F05%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本节课内容：1、神经网络的代价函数，及求代价函数中对应每个参数的偏导数的反向传播算法2、深入理解反向传播算法的思想3、使用梯度检测判断反向传播的实现是否正确4、随机初始化θ参数值5、整合上述所有内容，介绍训练神经网络的步骤 代价函数在神经网络中使用的代价函数是逻辑回归中使用的代价函数的一般形式对于逻辑回归而言，通常使代价函数J(θ)最小化，下图中的逻辑回归代价函数的正则项是j从1到n的求和形式，因为没有把偏差项θ^0 正则化对于一个神经网络来说，代价函数就会是逻辑回归代价函数的一般形式，不再仅有一个逻辑回归输出单元，而是K个，此时的求和项需要计算K个输出单元之和e.g. 如果我们有四个输出单元，即神经网络的最后一层有四个输出单元，那么求和项就需要求k从1到4每一个逻辑回归算法的代价函数，然后按四次输出的顺序依次把这些代价函数加起来代价函数的正则化项则是对θ_ji^((l)) 项所有i、j、l值求和，和逻辑回归的代价函数一样，这里要除去那些对应于偏差值的项，即不对i=0的项求和，当我们计算神经元的激励值时，i=0的项会对应乘上某个x_0或a_0的项，类比于逻辑回归的代价函数，我们不会将这些项加到正则项中，但即使真的加进去，依然是有效的，且不会有大的区别 反向传播算法前向传播算法使我们可以计算神经网络中每一个神经元的激活值，为了计算导数项，要采用反向传播算法反向传播算法从直观上讲，对于每一个神经元，计算δ_j^((l))，即第l层第j个节点的激活值的误差以下图中的4层神经网络为例，对于每一个输出单元，计算δ项，所以第四层第j个单元的δ等于这个单元的激活值减去训练样本里的真实值，也可以看成假设的输出值与训练样本y值的差下一步要计算神经网络中的前几层的误差项δ，这里不计算δ^((1))，因为第一层是输入层，表示在训练集中观察到的，不存在误差 反向传播算法，是从输出层开始计算δ项，然后返回上一层计算第三层隐藏层的δ项，再往前计算δ^((2))，这类似于把输出层的误差反向传播给了第三层，然后再传到第二层，这就是反向传播的意思可以简单地证明（忽略正则项），代价函数的偏导数是激活值和δ项的乘积反向传播算法可以快速计算出所有参数的偏导数项 反向传播算法的实现：假设有m个样本的训练集：1、设置下标为Δ_ij^((l)) 的初始值为0，其会被用来计算J(Θ)对应Θ_ij^((l)) 的偏导数项2、遍历训练集 2.1 利用正向传播计算每层神经节点的激活值 2.2 计算最后一层输出值与训练集中y值的误差δ^((L)) 2.3 运用反向传播算法，计算δ^((L−1)) 、δ^((L−2))… δ^((2))，不计算δ^((1))，因为不需要对输入层考虑误差项 2.4 通过累加，计算偏导数项Δ_ij^((l))3、计算公式的D值，如下图所示计算出D值后，它正好就是代价函数关于每个参数的偏导数，然后就可以使用梯度下降或者其他高级优化算法 理解反向传播前向传播过程：假设有一个如下图的神经网络，通过前向传播计算每一层中各个神经节点的z值和a值（激活值，输入层中样本原始值即激活值），Θ值为由前一层的神经节点指向后一层的神经节点的箭头的权值z值的计算方法为前一层a值的加权和，权重为相应Θ值 反向传播做了什么？考虑只有一个输出单元，忽略正则项，这时y^(i)为实数，下图中J(Θ)公式中蓝线的部分对应一个样本点(x^(i), y^(i))的代价值计算，即cost(i)的计算，可以将复杂的的公式简单理解为方差，表示神经网络输出值与实际观测值y^(i)的接近程度 反向传播过程：一种简单的理解就是计算所有的δ项，即每个神经元的误差更正式地，δ项实际上是代价函数cost(i)关于z_j^((l)) （中间项）的偏导数，它衡量的是为了影响这些中间值，我们想要改变神经网络中的权重的程度，进而影响整个神经网络的输出h(x)，并影响所有代价函数反向传播的计算与前向传播非常类似，指示方向反了过来，下图以计算δ_2^((2)) 为例展示了反向传播计算δ项的过程（彩色字）此外这个δ项的计算过程并不包括偏置单元，偏置单元δ项的计算取决于对反向传播的定义，以及实现算法的方式，也可以用 其他方法计算偏置单元的δ项，偏置单元的输出总是+1，他们最后不会影响偏导数的计算 梯度检测反向传播算法在实现时会有一些bug，当它与梯度下降或其他算法一同工作时，看起来确实能正常运行，而且代价函数J(Θ)在每次梯度下降的迭代中也在不断减小，但最后得到的神经网络的误差将比无bug的情况下高出一个量级而且你有可能不知道你得到的结果是由bug所致的为解决这种问题，可以使用梯度检验，他将完全保证你的前向传播以及反向传播都是百分之百正确的，而且，上述问题的出现绝大多数情况下都和反向传播的错误实现有关 考虑下面这个例子，假设有一个代价函数J(Θ)，其中Θ为实数，想估计函数在某一点上的导数，该点导数就是图像在该点上切线的斜率，现在要从数值上逼近它的导数，或者说从数值上来求近似导数首先计算θ+ε和θ−ε，把对应的点用直线连接起来，图中红线斜率就是该点导数的近似值，蓝线斜率是该点导数的确切值接下来考虑Θ更普遍的情况，即为向量参数的时候，设Θ是n维向量，它可能是神经网络中参数的展开式我们可以用类似的思想来估计所有偏导数项通过计算J(Θ)对应每个Θ的近似偏导数，将其与在反向传播中得到的梯度进行比较，如果这两个结果是否相等或非常接近，就可以说反向传播的实现是正确的 总结：1、通过反向传播计算Dvec，DVec可能是矩阵的展开形式2、实现数值上的梯度检验，计算出gradApprox3、确定DVec和gradApprox都能得出相似的值4、在使用代码进行学习或训练网络之前，关掉梯度检验，不要再使用导数计算公式来计算gradApprox，因为梯度检验的代码计算量非常大，也非常慢，相对的，反向传播算法是一个高性能的计算导数的方法，所以一旦通过检验确定反向传播的实现是正确的，就应该关掉梯度检验不再使用 随机初始化当执行一个算法，如梯度下降法或高级优化算法时，我们需要为变量Θ取一些初始值，高级优化算法中会默认给Θ提供一些初始值，对于梯度下降法，同样的也需要对Θ进行初始化，之后就可以一步一步通过梯度下降，来最小化代价函数J一种想法是将Θ的初始值全部设为0，尽管在逻辑回归中是允许的，但在训练网络时这起不到任何作用假设下图中的一个神经网络，如果将Θ全部设为0，图中相同颜色线条上的权重均相等，即0，对于所有的训练样本，最后总能得到a_1^((2))=a_2^((2))，进一步计算会得到δ_1^((2))=δ_2^((2))如果继续把图补全，会发现代价函数关于这些参数的偏导数会满足这样的关系，如：蓝色线条对应的权重的偏导数相等，这意味着在每一次梯度下降更新中，蓝色线条的权重仍会相等，也就是相同颜色线条的权重仍然相等，继续进行梯度下降，中间隐藏单元的激活值（a值）仍然相等想象一下你有不止两个隐藏单元，而是有很多很多隐藏单元，也就是说所有的隐藏单元都在计算相同的特征，所有的隐藏单元都以相同的函数作为输入，这是一种高度冗余的现象，这意味着最后的逻辑回归单元只能得到一个特征，因为所有的单元都一样，这种情况阻止了神经网络去学习任何有趣的东西上述问题被称为对称权重问题，也就是所有的权重都是一样的，为解决这种问题，在神经网络中要使用随机初始化的思想，对于Θ中的每一个值，将其设为一个范围在−ε到ε之间的随机值因此，在初始化Θ时，要将其中的值初始化为接近0，范围在−ε到ε之间的数，然后进行反向传播，再进行梯度检验，最后使用梯度下降或者其他高级优化算法来最小化代价函数J 组合到一起选择网络架构，即神经元之间的连接模式输入层：首先定义输入单元的数量，一旦确定了特征集x，输入单元的数量就等于特征x^((i)) 的维度，输入单元数目将会由此确定 输出层：如果进行多分类，那么输出层的单元数目，将会由分类问题中所要区分的类别个数确定。如果多元分类问题y的取值范围是在1到10之间，那么就可能有10个分类，需要把输出y重新写成向量的形式 隐藏层：对于中间的隐藏层的层数以及每层的单元个数，一个合理的默认选项是只使用单个隐藏层，如果使用不止 一个隐藏层，通常每个隐藏层都应有相同的单元数。但实际上通常来说，图中最左侧的结构是较为合理的默认结构，对于隐藏单元的个数，通常情况下，隐藏单元越多越好，不过如果有大量隐藏单元，计算量一般会比较大。此外，每个隐藏层的单元数量还应该和输入x的维度相匹配，即和特征的数量相匹配，隐藏单元的数目可以和输入特征的数量相同，或者它的二倍，或者三四倍 训练神经网络的步骤1、随机初始化权重。将权重初始化很小的值，接近于02、执行前向传播算法。即对神经网络的任意一个输入x^((i))，计算出对应的〖hx〗^((i)))值，也就是一个输出值y的向量3、计算代价函数J(Θ)4、执行反向传播算法，来计算J(Θ)关于参数Θ的偏导数项。我们要遍历所有训练样本x^((i))，将其输入到输入层，执行前向传播和反向传播，然后就能得到每个神经元的激励值a和δ值，进而计算偏导数5、使用梯度检查，将使用反向传播得到的导数值和使用数值方法得到的导数值进行比较，确保两种方法得到基本接近的两个值，从而确保我们的反向传播算法得到的结果是正确的。然后停用梯度检查，因为计算量大，计算非常慢6、使用最优化方法，如梯度下降或者更加高级的优化算法，将这些优化方法和反向传播结合，来得到使代价函数J(Θ)最小化的参数Θ 理解梯度下降在神经网络中的应用假设一个包含两个参数的代价函数，其图像如下图所示。代价函数度量的就是神将网络对训练数据的你和情况，假设取代价函数值最小的一点，即图像上最低的点，这一点所对应的一组参数对于大部分训练样本，假设函数的输出会非常接近y^((i))，也就是神经网络对训练数据拟合的较好因此，梯度下降的原理就是从某个随机的初始点开始，它将会不停的往下下降，那么反向传播算法的目的就是算出梯度下降的方向，而梯度下降的作用就是沿着这个方向一点点下降，一直到我们希望得到的点因此，当执行反向传播和梯度下降或者其他更高级的优化方法时，这幅图片解释了基本原理，也就是试图找到某个最优的参数使得神经网络的输出值与训练集中观测到的y^((i)) 的实际值尽可能的接近 课程资料课程原版PPT-Neural Networks: Learning练习内容和相关说明相关数据ex4data1，相关数据ex4weights python代码实现1.neural_network.ipynbML-Exercise3.ipynb]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（六）-神经网络学习]]></title>
    <url>%2F2019%2F08%2F27%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[本节课内容：1、介绍非线性假设，逻辑回归方法的局限性2、讲解神经网络的来源、理念、模型3、简单介绍神经网络的多元分类 非线性假设给定如图所示的数据集，我们可以构造包含许多非线性项的逻辑回归函数，当多项式数量足够多时，就可以分开正样本和负样本的假设。当只有两个特征时，可以将x1,x2的所有组合都包含进多项式中，但大多数机器学习问题的特征远不止两项（e.g. n=100），这时，如果要多项式中包含所有的项，即使只包含二次项，最终也会有很多项。因此要包含所有的二次项，并不是一个好方法，由于包含的项数太多，最终可能导致过拟合问题，而且处理这么多项也存在运算量过大的问题。当然也可以取二次项的子集，如只包含平方项，项的数量会明显减少，但最终不会得到理想的结果，因为忽略了太多相关项e.g. 在计算机视觉中，每个像素点看作一个特征，假设一个50x50的灰度图片，就包含2500个特征（如果是RGB图片则包含7500个特征），如果用逻辑回归，那么多项式中要包含3百万个项 神经元与大脑神经网络：起源：人们想尝试设计出模仿大脑的算法，它的理念是如果我们要建立学习系统，要去模仿我们所认识的最神奇的学习机器，人类的大脑神经网络兴起于20世纪八九十年代，应用的非常广泛，但由于各种原因，90年代的后期就应用的很少了，但最近神经网络又再度兴起，原因之一是神经网络的计算量较大，近些年计算机的运行速度变快，才足以运行大规模的神经网络 神经重接实验：如果有一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些，也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它，让大脑自己学习如何处理这些不同类型的数据 模型展示（一）大脑中的神经细胞在一个人工神经网络里，我们使用一个很简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元（图中黄色圆圈），通过输入通道传递给它一些信息，然后神经元做一些计算，再通过输出通道输出计算结果x_0称为偏置单元或偏置神经元，非线性函数g(z)称为激活函数，θ称为模型参数或权重神经网络是一组神经元连接在一起的集合神经网络的第一层称为输入层，在这一层输入特征；最后一层称为输出层，这一层输出最终计算结果；中间第二层称为隐藏层，隐藏层的值在训练集中是看不到的，神经网络可以有不止一个隐藏层，实际上任何非输入层和非输出层都成为隐藏层在神经网络中，a_i^((j)) 表示第j层第i个神经元或单元的激活项，激活项是指由一个具体神经元计算并输出的值一般地，如果一个神经网络，在第j层有s_j个单元，在第j+1层有s_(j+1)个单元，Θ^((j)) 即控制第j层到第j+1层映射的矩阵，它的纬度为s_(j+1)*(s_j+1)总结：神经网络定义了函数h，从输入x到输出y的映射，这些假设被参数化，记做Θ，因此只要改变Θ就能得到不同的假设 模型展示（二）前向传播：从输入单元的激活项开始，然后进行向前传播给隐藏层，计算隐藏层的激活项，继续向前传播，计算输出层的激活项，这个依次计算激活项，从输入层到隐藏层再到输出层的过程叫做前向传播下图中右侧为前向传播公式的向量化在下图的神经网络中，将左侧输入层临时遮住，图中剩下的部分很像标准的逻辑回归，但输入该逻辑回归的特征是通过隐藏层计算的新的特征a，而不是原本的特征x在神经网络中，新的特征项a是学习得到的函数输入值，即从第一层映射到第二层的函数，这个函数由参数Θ^((1)) 决定，因此在神经网络中没有用特征x做逻辑回归，而是训练逻辑回归的输入a，根据为Θ^((1)) 选择不同的参数，可以学习到一些有趣和复杂的特征，得到更好的假设函数，比使用原始特征x得到的假设更好神经网络中神经元的连接方式称为神经网络的架构 例子与直觉理解（一）神经网络中的单个神经元用来计算逻辑函数下图中左侧为右侧问题的简化版，是XNOR运算的训练集要构建神经网络拟合XNOR运算，要先从比较简单的能够拟合AND运算、OR运算和NOT运算的神经网络入手，下图是AND运算的神经网络 例子与直觉理解（二）将输入放在输入层，中间放一个隐藏层用来计算一些关于输入的略微复杂的功能，然后再继续加一层用来计算一个更复杂的非线性函数当神经网络有许多层，在第二层中有一些关于输入的相对简单的函数，第三层又在此基础上计算更加复杂的方程，再往后的层次计算的函数越来越复杂 多元分类如下图所示的神经网络要来识别图片中的物体，在输出层中，4个分类器每个都将识别图片中的物体是否是四种分类中的一种，即四输出单元的神经网络之前我们用整数y作为输出的分类标签，现在用向量y^(i)作为输出结果，y^(i)的值取决于对应的图像x^(i)，因此，一个训练样本由一组(x^(i), y^(i))组成，其中x^(i)是四种物体其中一种的图像，y^(i)是结果向量中的一个，我们要找到一些方法让神经网络输出一些数值，输出值h(x^(i))等于y^(i)，并且h(x^(i))与y^(i)在本例子中都是四维向量，分别代表不同的类别 课程资料课程原版PPT-Neural Networks: Representation练习内容和相关说明相关数据ex3data1，相关数据ex3weights python代码实现1.neural_network.ipynbML-Exercise3.ipynb 向量化标签]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（五）-正则化]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本节课内容：1、介绍什么是欠拟合和过拟合2、介绍过拟合问题的解决方法——正则化3、介绍线性回归和逻辑回归中，正则化的应用 过拟合问题以估计房屋价格的问题为例，给定一些数据（如下图所示），随着size的增大，房屋的价格是趋于稳定的如果用直线来拟合数据（左图），和这样的趋势不相符，所以不能很好的拟合训练数据，这个问题称为欠拟合（underfit），或称算法具有高偏差（high bias）如果用二次函数拟合数据（中间图），拟合效果很好另一种极端情况（右图），拟合一个四阶多项式，这似乎很好的拟合了数据集，因为图像经过了所有的数据点，但这是一条扭曲的曲线，不停的上下波动，我们不认为这是一个预测房价的好模型，这个问题称为过拟合（overfit），或称算法具有高方差（high variance）过拟合（Overfitting）：过拟合问题会在变量过多的时候出现，这时训练出的假设函数（hypothesis）能很好的拟合训练集（代价函数可能非常接近0），它千方百计的拟合训练数据集，导致无法泛化到新的样本中泛化：指一个假设模型应用到新样本的能力同样的问题也存在于逻辑回归中解决过拟合问题当我们使用一维或二维数据时，可以通过绘制出假设模型的图像来研究问题。但实际应用中，机器学习问题需要很多变量，这不仅仅是选择多项式阶次的问题，当我们有很多特征变量时，绘图会变得更难，通过数据可视化来决定哪些特征变量也会更难如果有很多特征变量，而只有非常少的训练数据时，就会出现过拟合问题 有两个办法解决问题：1、减少选取变量的数量： - 人工检查变量清单，并决定哪些变量更重要，哪些特征应该保留，哪些应该舍去 - 模型选择算法，自动选择哪些变量保留，哪些舍弃缺点：舍弃一些特征变量的同时，也舍弃了关于问题的一些信息2、正则化： - 保留所有特征变量，但减少量级或参数θ_j 的大小 - 当有许多特征变量，且每个特征变量都对预测y值有影响时 ，效果很好 代价函数向代价函数中加入惩罚项，使得θ_3，θ_4 都非常小（如下图所示）正则化：如果参数值较小，意味着更简单的假设模型（越平滑），更不容易出现过拟合问题 e.g.假设给定一个数据集中有很多特征变量（m个），我们并不知道那个参数对应高阶项，对于m个特征，我们也很难选出其中哪个变量是相关度较低的，也就是有n个参数θ，很难选出哪些参数来缩小它的值，我们要做的就是修改代价函数来缩小所有参数（没有对θ_0 添加惩罚项，这是约定俗成的，实际上无论从θ_0 开始，还是θ_1 开始，对结果影响都不大）λ为正则化参数，作用是控制两个不同目标之间的平衡关系。第一个目标与目标函数的第一项有关，即更好的拟合训练数据；第二个目标是保持参数尽量小，与目标函数第二项（正则化项）有关在正则化线性回归中，如果正则化参数λ被设置的太大，结果就是对参数的惩罚程度太大，会导致参数都接近于0，相当于把假设函数的全部项都忽略掉了，最后假设模型只剩下h(x)=θ_0，这就导致欠拟合因此，为了让正则化起到效果，要选择更合适的正则化参数λ 线性回归的正则化梯度下降将θ_0 单独分离出来（正则化从θ_1 开始）在正则化的梯度下降中，每一次迭代，θ_j 都会乘一个比1略小的数，也就是θ_j 每次迭代都会缩小一点点，然后进行和之前一样的更新操作正规方程对于X’X矩阵是否可逆的问题：在正规方程的正则化中，只要λ是严格大于0的，就可以确信公式中括号内的矩阵一定是可逆矩阵，因此正则化还可以解决X’X出现不可逆的问题 逻辑回归的正则化 课程资料课程原版PPT-Regularization练习内容和相关说明相关数据1，相关数据2 python代码实现1.logistic_regression_v1.ipynbML-Exercise2.ipynb]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（四）-逻辑回归]]></title>
    <url>%2F2019%2F07%2F30%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[本节课内容：1、介绍分类问题，并用逻辑回归解决分类问题2、介绍逻辑回归的决策边界，假设函数（hypothesis），代价函数，以及用梯度下降法求解最优θ值3、介绍高级优化方法，及其在octave中的实现4、介绍多分类问题（一对多，一对余），以及解决多分类问题的基本思想 分类y的取值是离散的二分类问题中，yϵ{0,1}，0表示“负类”，1表示“正类”，在两个类别中，哪个是正类哪个是负类并没有什么区别，但一般来说，负类表示没有某样东西 给定一个分类问题的训练集（如下图所示），如果用线性回归的方法对数据进行拟合，会得到一个分界点，其左侧预测为0，右侧预测为1，在这个数据集上，线性回归的结果似乎是可行的假设数据集改变，如下图所示，此时，如果再用线性回归来拟合数据，会得到一条新的直线，分界点也会随之改变，其左侧预测为0，与原数据不相符分类问题中（此节讨论二分类问题），y的取值为0和1，但线性回归的预测结果可能是远大于1，或远小于0因此，将线性回归用于分类问题中是不合理的 逻辑回归：预测值一直介于0,1之间分类算法 假设陈述解释：h(x)的输出是，对于一个输入x，y=1的概率估计 决策界限当θ^T x≥0时，预测y=1；当θ^T x&lt;0时，预测y=0e.g.假设给定一个数据集，得出一个拟合好的逻辑回归函数，如下图所示当-3+x1+x2≥0时，预测y=1；当-3+x1+x2&lt;0时，预测y=0-3+x1+x2=0这条直线叫做决策边界决策边界是假设函数（hypothesis）的属性，取决于其参数，不是数据集的属性 非线性决策边界e.g.假设给定一个数据集，得出一个拟合好的逻辑回归函数，如下图所示 代价函数如果使用线性回归中的平方函数定义代价函数，得到的代价函数是非凸的（non-convex），因为h(x)是非线性的，这样就会有多个局部最优解，使用梯度下降法，无法保证得到全局最优解因此需要另找一个不同的代价函数，它是凸函数，使我们可以使用很好的算法，如梯度下降，能保证找到全局最小值 逻辑回归的代价函数 简化代价函数与梯度下降逻辑回归的梯度下降规则，与线性回归的梯度下降规则，几乎一摸一样，但由于假设函数不同，其与线性回归的梯度下降是两个完全不同的东西 高级优化介绍一些高级优化算法和高级优化概念，这些方法与梯度下降相比，能大大提高逻辑回归运行的速度，使算法更加适合解决大型的机器学习问题其他优化算法：共轭梯度（Conjugate gradient），BFGS，L-BFGS优点：1、不需要手动选择α的值：这些算法给出计算导数项和代价函数的方法，可以理解为这些算法有一个智能内循环（clever inner-loop）， 称为线搜索算法，它可以自动尝试不同的学习率α并自动选择一个好的学习率α，甚至可以为每次迭代选择不同的学习率2、收敛得远远快于梯度下降：缺点：更复杂 多元分类：一对多一对多（一对余）的分类思想假设给定一个训练集（如下图所示），其中包含三个类别，我们要做的是将这个训练集转化为三个独立的二分类问题，得到三个假设函数（分类器），分别输出y=i时的概率估计最后为了做出预测，给出一个新的输入值x，在三个分类器上运行，然后选择h最大的类别也就是选择出三个中可信度最高、效果最好的分类器，无论i值是多少，都能得到一个最高的概率值，预测y就是那个i值 课程资料课程原版PPT-Logistic Regression]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Probabilistic Model for Using Social Networks in Personalized Item Recommendation]]></title>
    <url>%2F2019%2F07%2F29%2FA-Probabilistic-Model-for-Using-Social-Networks-in-Personalized-Item-Recommendation%2F</url>
    <content type="text"><![CDATA[ABSTRACT问题：传统的基于偏好的推荐没有考虑到用户的社交方面，一个值得信赖的朋友可能会指出我们一个与我们的典型偏好不符的有趣项目目的：缩小基于偏好的推荐和基于社会关系的推荐之间的差异方法：SPF（social Poisson factorization，社会关系矩阵泊松分解），一种将社交网络信息纳入传统分解方法的概率模型效果：在6个真实数据集上的性能优于其他推荐方法 INTRODUCTION研究表明，用户看中朋友对内容的发现和讨论意见，在线访问社交网络可以强化这种现象矩阵分解方法不能利用这些信息，他能推断出你可能喜欢的物品是因为这个物品符合你的偏好，但是不能推断出你朋友喜欢且你也可能感兴趣的物品本文提出基于贝叶斯矩阵分解的SPF（social Poisson factorization）方法，可以在社交层面反映用户消费的商品SPF假设有两个信号驱动用户点击：对项目的潜在偏好和朋友的潜在影响SPF推测每个用户的偏好和影响，向用户推荐即符合用户偏好又被其朋友点击过的项目 Related Work：过去研究的一些缺陷： 1、假设用户影响（也称信任）是可以直接获取的，但除了是/否这样的二进制信任信息之外，其他的信任信息用户是很难输入的 2、假设信任是从网络结构中传播或计算得到的，忽略了用户活动，这可以反应用户对网络某些部分的信任 3、通过计算用户之间的相似程度来计算信任，会使一般意义上的偏好相似度和用户影响的概念混淆，比如：两个拥有相同兴趣偏好的用户可能各自阅读同一本书，但彼此之间没有交集 4、将社交信息纳入协同过滤方法中，与矩阵分解方法相比，缺乏可解释性 5、探索了传统的矩阵分解方法如何利用社交关系，例如许多模型分解了user-item数据和user-user网络，使相连用户的潜在偏好更接近彼此，反映出朋友有着相似的喜好 e.g. 如果用户喜欢某个项目只是因为他许多朋友也喜欢，这完全超出了他通常的喜好 本文思路：寻找不同偏好的朋友（均衡朋友的影响），辅助向用户推荐不符合他通常喜好的物品。根据社交网络调整朋友的总体喜好，不允许用户仍然享受这种异常项目（朋友喜欢导致超出用户的通常喜好）的可能性 *注1：用户影响，即信任，是当两个用户是朋友有互动的前提下的，而通过偏好相似度计算得出的信任，很可能使非朋友的两个用户之间存在信任关系*注2：假设用户可能喜欢某个项目，喜欢的程度是A；该用户的朋友也喜欢该项目，考虑到朋友的影响，再结合本身的喜好，导致得出该用户对该项目的喜欢程度是B；结果其实B是高于A的 SOCIAL POISSON FACTORIZATION1、利用潜在用户偏好和潜在项目属性，捕捉用户的活动模式2、估计用户受到朋友可见点击（朋友喜好）的影响有多大3、推荐有影响力的朋友点击的项目，即使他们不符合矩阵分解得出的用户偏好 Background: Poisson factorization泊松矩阵分解，即将通常的矩阵分解方法中的高斯分布换成泊松分布 Social Poisson factorizationSPF的思想：用户喜欢一个item有两个原因：用户的喜好和item的属性相符合（泊松矩阵分解/其他矩阵分解的思想）；用户有喜欢这个item的朋友，或者他的许多朋友都喜欢这个item在PF（泊松矩阵分解）中，每个用户有一个潜在喜好的向量。但每个用户又有一个对每个朋友影响力的向量。一个用户是否喜欢某个item取决于她潜在的喜好和item潜在的属性，也取决于对她有影响的朋友是否点击了它 Model specification（模型设定） 可用数据是用户行为和社交网络 行为数据是系数矩阵R，其中r_ui 是用户u点击项目i的次数 社交网络通过邻居集合表示：N(u)是连接到用户u的其他用户的索引集合 隐藏变量是每个用户的非负偏好k维向量θ_u，每个item的非负属性k维向量β_i，每个邻居的非负影响τ_uv τ_uv 表示用户u多大程度上受到用户v点击的影响 给定矩阵R的联合分布： Forming recommendations with SPF（通过SPF形成推荐） Learning the hidden variables with variational methods（用变分方法学习隐变量）在给定了点击数据和社交网络的条件下，我们的目的是计算用户偏好、项目属性和潜在影响的后验分布对于许多贝叶斯模型，SPF的精确后验分布是很难计算的变分方法被广泛用于统计机器学习来适应复杂的贝叶斯模型本文提出了基于变分方法的近似推理算法，通过这个算法，可以在非常大的数据集上计算近似后验期望 变分推理通过解决最优化问题逼近后验1、在隐变量上定义自由参数化分布，将其参数拟合为近似后验分布2、使用Kullback-Leibler散度来度量“近似度”，是一种对分部间距离的不对称度量3、用拟合的变分布替代后验分布 EMPIRICAL STUDY（实验研究） 将SPF与五种涉及推荐中的社交网络的方法，和两种传统的矩阵分解方法，进行了比较 在6个真实数据集上，我们的方法优于以上所有方法 展示了如何利用SPF对数据进行挖掘，并从潜在因素和社会影响两个方面对数据进行了表征 评估了对潜在因素数量的敏感性，并讨论了如何在先验分布上设置超参数 数据集、方法和度量6个数据集：1、Ciao (ciao.co.uk)：具有潜在社交网络的消费者评论网站，7K用户和98K项目的DVD评分和信任值2、Epinions (epinions.com) ：消费者评论网站，用户在网站上对商品进行评分，并将用户标记为可信赖的，包含39K用户和131K项目3、Flixster (flixster.com)：社交电影评论网站，我们将评分二值化，阈值设置在3或以上，从而得到132 K用户和42K项目4、Douban (douban.com)：一家中国社交网站，记录音乐、电影和书籍的评分，包含129K用户和57K项目5、Etsy (etsy.com)：一个手工制品和古董以及艺术品和工艺品的市场，用户可以互相关注，并将项目标记为收藏，这些数据由Etsy直接提供，筛选出了那些至少收藏了10项，并且至少有25%的项目与他们的朋友有共同之处的用户，我们省略了少于5个收藏的项目，包含40K用户和5.2M项目6、Social Reader：一家大型媒体公司的数据集，在流行的在线社交网络上部署了一个阅读应用，数据包含一个社交网络和一个文章点击表，我们分析了2012年4月2日-6日的数据，仅包括在此期间至少阅读了3篇文章的用户，包含122K用户和6K项目 下表概述了6个数据集的属性预处理：对于隐式数据，非泊松模型要求我们对0进行子采样，以便区分项目；在这些情况下，我们随机抽样负示例，使每个用户都具有相同的正面和负面评分。请注意，基于泊松的模型隐式地分析了整个矩阵，而不需要支付分析零的计算成本对于每个数据集，我们删除了用户没有共同项的网络连接。请注意，这对SPF和比较模型都有好处(虽然SPF可以了解邻居的相对影响)将数据分为三组：约10%的用户数据用于推理后测试中（approximately 10% of 1000 users’ data），1%的用户数据用来评估推理的收敛性，其余的用于训练。一个例外是Ciao，我们使用了10%的用户数据进行测试（10% of all users’ data） 比较方法：与以下几个模型进行比较：RSTE，TrustSVD，SocialMF，SoRec，TrustMF，以及概率高斯矩阵分解（PMF，广泛使用的推荐方法），对于每一个模型，采用最佳参数设置，与LibRec网站上发布的示例拟合SPF有两个部分：泊松分解分量和社会分量，因此将这两部分单独比较：泊松分解(PF)和社会因素分解(SF)比较两个标准，随机订购项目和根据普遍受欢迎程度订购项目 度量（NCRR）：对于每个用户，分别预测被搁置（held-out）项目的点击量，和真正未点击项目的点击量，并通过他们的期望值进行排名，好的模型应将搁置的项目排在前面 下图为实验结果 Performance and exploration（性能与探索）性能：各模型表现如图3所示，本文提出的SPF在所有6个数据集上的表现都优于其他模型，除了在Etsy上，纯社会因素分解(SF)表现最好。此外，基于流行度排序的表现良好，这突出了社会因素分解的重要性，只有SPF的表现一直优于它在Ciao数据集上测量运行时，以了解相关的计算成本。下图显示了所有方法在K的不同值下的运行时，泊松模型在运行时方面是平均的在Ciao和Epinionis数据集上，将SPF、SF和PF的性能看做每个用户的度的函数；结果如下图所示。所有的模型在高度的用户上都表现得更好，大概是因为他们的活动度更高。SPF的性能优于SF，因为它在大量的低度的用户上有优势 Interpretability（可解释性）：使用辅助变量将每个推荐归因于朋友或一般偏好，然后使用这些属性来探索数据下图显示了在Ciao和Epinions数据集上，社会归因（与一般偏好归因相对）的比例如何随着用户度的变化而变化，观察到，Epinions将更大部分的行为归因于社会影响，并受用户度的控制下图展示了社会影响与用户入度间的函数关系，在Epinions数据集上，高入度的用户比低入度的用户的社会影响更低 实验细节潜在因子k值的选择：所有的矩阵分解模型，包括SPF，都要求选择用于表示用户和项目的潜在因子K的值，我们评估了Ciao数据集对K的敏感性，下图展示了K从1到500的平均NCRR，SPF在K=40时表现最佳，但对K敏感性低于其他一些方法（如PF）超参数：必须将超参数设置为潜在变量的γ先验， γ通过形状和速率参数化，将其设置为0.3，用于潜在偏好和属性的先验。 将先前用户影响的超参数设置为（2,5），为了鼓励模型通过社会影响来探索解释。 在一项试点研究中，我们发现该模型对这些设置不敏感。 在冷启动的情况下，我们知道用户的社交网络，但不知道他的点击项目，执行SPF相当于执行SF，SPF在这种冷启动的场景下比其他模型（本文提到的几个模型）表现都好 DISCUSSION本文提出社会泊松分解（SPF），一种贝叶斯模型，结合了用户对其朋友潜在影响的项目的潜在偏好，我们证明，SPF即使在有噪声的社会网络上，也能改善推荐SPF有以下特性：1、它揭示了社交网络中用户之间存在的潜在影响，使我们能够分析社会动态。2、它提供了可解释的意外发现的来源（？？？）3、它具有可扩展的算法，适用于大数据集我们的模型没有考虑到时间（当两个相连的用户都享受一个项目时，其中一个可能首先使用它） 论文原文A Probabilistic Model for Using Social Networks in Personalized Item Recommendation]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>RecSys2015</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（三）-多变量线性回归]]></title>
    <url>%2F2019%2F07%2F26%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[本节课内容：1、多变量线性回归的Hypothesis公式、参数、代价函数。2、特征缩放、归一化处理、以及学习率α值的选取。3、特征的选取和多项式回归。4、另外一种得到θ最优解的方法-正规方程，以及正规方程中X’X不可逆的情况和解决方法。 多变量线性回归 特征缩放（Feature Scaling）不同特征的取值在相近的范围内，梯度下降法就能更快的收敛 e.g. 假设要根据房屋的面积和卧室的数量预测房屋的售价，房屋的面积取值范围是0-2000，卧室的数量的取值范围是1-5，画出代价函数的等值线（忽略θ_0），图形是很狭长的，这样在梯度下降找到全局最小时，需要很长的时间，且来回波动。如果将特征进行缩放，把x_1 定义为房屋面积除以2000，把x_2 定义为卧室数量除以5，那么代价函数的等值线就不会很狭长，会更圆一点，在这样的代价函数上进行梯度下降，会找到一条更直接的路径通向全局最小 特征缩放是将特征的取值通过乘除限制在某个合适的范围内（不一定必须是-1~1之间），以使梯度下降更快的收敛，使迭代次数更少 归一化处理使用x_i−μ_i 代替x_i，使特征值取值的平均值为0不用将归一化应用于特征x_0，因为x_0 的取值始终为1 将归一化和特征放缩相结合图中μ_i 的取值为特征值的平均值，s_i 的取值为特征值的范围（max-min）或者标准差 代价函数曲线如果梯度下降算法正常工作，每一步迭代后的代价函数都应该下降代价函数曲线可以判断是否收敛，以及梯度下降在迭代多少次后收敛 自动收敛测试：如果代价函数在一步迭代后的下降小于一个很小的值ϵ，即判断函数已经收敛实际上，选择一个合适的阈值ϵ还是很困难的，所以（老师）更倾向于看代价函数曲线来判断收敛，而不是用自动收敛测试 代价函数也可以提前警告算法没有正常工作：1、如果代价函数的值在不断上升，通常意味着应该使用较小的学习率α最常见的原因是（代码中无错误的情况下），在尝试最小化一个碗型函数（图中粉色函数）时，学习率太大，梯度下降算法可能会不断冲过最小值2、代价函数值不断的下降再升高，也是学习率α过大 数学家已经证明，只要学习率α足够小，每次迭代后的代价函数都会下降但如果学习率α取值非常小的话，梯度下降算法可能收敛的很慢 综上所述：-如果学习率α取值非常小，梯度下降算法可能收敛的很慢-如果学习率α取值非常大，代价函数可能在迭代后不下降；最后可能不收敛；也可能收敛很慢通常选择α取值时，尝试一系列α值，直到找到一个使算法正常工作的最小的α值，和一个最大的α值，然后取最大的α值，或者比最大α值略小一点的比较合理的值 特征和多项式回归1、特征：当使用线性回归时，不一定直接使用给出的特征，可以自己创造新的特征e.g.给定的特征是房屋的宽度和深度，但我们可以选择将宽度和深度的乘积，即面积，作为特征2、多项式回归：对于一个数据集，可能会有多个不同的模型进行拟合，当直线不能很好的拟合数据时，可以使用二次模型、三次模型等。可以使用多元线性回归的方法，对多元线性回归算法稍作修改，来实现二次模型、三次模型等。3、当使用多项式回归时，特征放缩就变得更重要4、凭着对各种函数图形到的了解，以及对数据形状的了解，通过选择不同的特征，可以得到更好的模型 正规方程一种求θ的解析解法，一次性求解θ的最优值，无需迭代使用正规方程，不需要进行特征放缩 正规方程与梯度下降的优缺点：当n较小时，选择正规方程；当n较大时，选择梯度下降，因为正规方程中求取逆矩阵的时间复杂度太高正规方程不适合用于更复杂的学习算法中 正规方程中X’X不可逆的情况（X’为X的转置矩阵）1、多余的特征（线性相关）e.g.在预测房屋价格时，x1为房屋的面积（平方米），x2为房屋的面积（平方英尺），那么x1和x2之间存在一个线性关系，这时的特征矩阵X’X不可逆2、特征太多（e.g.m≤n）可以删除某些特征，或正则化 课程资料课程原版PPT-Linear Algebra review (optional)课程原版PPT-Linear Regression with multiple variables]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（二）-单变量线性回归]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[本节课内容：介绍单变量线性回归的Hypothesis公式、参数、代价函数，通过梯度下降法计算最优参数值，得到最终的Hypothesis公式，进行预测 单变量线性回归代价函数（Cost Function）：平方误差函数（最常用） 梯度下降广泛用于机器学习的众多领域α称为学习速率，控制以多大的幅度更新参数：当α很小时，梯度下降很慢当α很大时，梯度下降可能越过最低点，可能无法收敛，甚至会发散 注意：θ_0 和θ_1 必须同步更新 在梯度下降法中，当我们接近局部最低点时，梯度下降会自动采取更小的幅度，因为局部最低点的导数为0，当接近局部最低点时，导数值会自动变得越来越小 线性回归的梯度下降“Batch” Gradient Descent：每一步的梯度下降的计算中都涉及到训练样本中的全部数据 课程资料课程原版PPT-Linear regression with one variable练习内容和相关说明相关数据1，相关数据2 python代码实现1.linear_regreesion_v1.ipynbML-Exercise1.ipynb]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习课程笔记（一）-初识机器学习]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E5%88%9D%E8%AF%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[机器学习定义：1、在没有明确设置的情况下，使计算机具有学习能力的研究领域（Arthur Samuel 1959, 跳棋游戏）2、一个适当的学习问题定义如下，计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过 P测定在T上的表现因经验E而提高（Tom Mitchell 1998） 机器学习算法:监督学习、无监督学习（最常用的两种）强化学习、推荐系统 监督学习：给算法一个数据集，其中包含了正确答案，算法的目的是给出更多正确答案连续值：回归问题，即选择合适的函数拟合数据离散值：分类问题，可能输出多个结果 无监督学习给算法一个数据集，其中不包含标签信息或者标签相同，算法的目的是找到其中的某种结构 聚类算法：根据数据的分布特征，将数据分成不同的簇应用：google新闻、基因组学、组织大型计算机集群、社交网络、市场划分、天文数据分析等 鸡尾酒会算法：从混合音频源中分离音频 课程资料课程原版PPT-Introduction]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Calibrated Recommendations]]></title>
    <url>%2F2019%2F07%2F23%2FCalibrated-Recommendations%2F</url>
    <content type="text"><![CDATA[Abstract校准（calibration），在机器学习公平性的背景下被重新重视保证准确率的推荐，很容易导致： 1. 用户的次要兴趣被主要兴趣挤出，使得推荐结果的领域逐渐趋向狭窄 2. 当多个用户使用同一账号时，不活跃的用户的兴趣会被排挤校准推荐（calibrated recommendations）可以避免这个问题贡献：概述量化标准度的度量指标，后处理推荐系统输出的重新排序算法 INTRODUCTION校准（calibrated）：各个类别的预测比例与数据中的实际比例一致校准推荐（calibrated recommendations）：推荐列表能够反应用户的各种兴趣比例校准的多样性：不同于项目之间最小相似性意义上的多样性，和推荐项的冗余 MOTIVATING EXAMPLE离线数据集：由用户-项-交互历史数据组成，将其分为训练集和测试集目的：保证最高准确度的情况下，在测试集中预测用户的交互项 假设：一个用户播放了70部“浪漫类”电影，和30部“动作类”电影 电影类别互斥目的：生成一个包含10部电影的推荐列表 类别不均衡：最极端情况：只知道用户对电影类别的喜好，没有任何其他电影的信息为达到最高准确度，会100%为用户推荐“浪漫类”的电影，此时准确率为70%如果随机为用户推荐电影，70%的概率推荐“浪漫类”，30%的概率推荐“动作类”，准确率只有0.7 · 70% + 0.3 · 30% = 58%因此，在保证准确性的前提下： 1. 在推荐结果中，用户的主要兴趣领域会被放大，而有轻微兴趣的领域会被排挤出去 2. 推荐结果是存在偏向的，偏向于用户的主要兴趣领域反过来说，为提高平衡性，或者校准推荐，会降低推荐的准确性 考虑电影概率： Latent Dirichlet Allocation（LDA）：受LDA的启发，用户选择电影的两步过程：选择类型，选择电影提到LDA的原因： 1. 假设在现实世界中，用户确实按照以上两个步骤选择电影。LDA模型在训练时，能够以正确的比例捕获用户的兴趣平衡。推荐列表是迭代生成的，每次迭代增加一个推荐项，推荐项的得出遵循它的生成过程，即上述两个步骤：首先，类型g会根据p(g|u)被抽取出来；然后，电影i根据p(i|g)从类型g的所有电影中抽出。因此，即使电影i被选中的概率很小，也可以排进推荐列表的前方，但准确率比评分推荐低 2. 不平衡推荐的问题不仅限于显式类别，也适用于潜在主题或嵌入情况，LDA就是这样的模型 3. 不管电影是否属于一个类别，或属于几个类别，不平衡推荐的问题都会出现 CALIBRATION METRICS（校准度量） CALIBRATION APPROACHES（校准方法） RELATED CONCEPTS多样性（Diversity）：多样性（Diversity）和校准（Calibration）的区别： 1. 多样性不能直接反应用户的兴趣比例：多样性即推荐项间的最小冗余或最小相似性，使推荐结果不是100%的“浪漫类”电影（在前面的例子中）。如果只有两种类型的电影，最多样性的推荐是50%“浪漫类”，50%“动作类”；如果还有其他类型的电影，多样性的推荐会把用户没有播放过的电影类型也推荐给用户，但如果把“动作类”电影在推荐中的比例由0%提升到30%以反映用户的兴趣比例，就不能保证多样性。只有把多样性和准确性平衡好，才能达到良好校准推荐。对于每个用户，这种折中的程度都是不同的。 2. 多样性可以推荐用户兴趣之外的电影，避免了推荐范围的减小（马太效应），而校准推荐没有这样的特性。这激发了对校准推荐的简单扩展，即用户兴趣之外的类别也会加入到推荐列表中：p_0 (g)表示每种类型的先验分布，取正数，为促进多样性DP：本质上是p(g│u)和q(g│u)之间的修正平方差，符合仅我们提到的特性1BinomDiv：符合特性2,3，劣势：无法给出推荐列表的校验程度（与目标比例的偏差），只能对不同推荐列表做相对比较；由于每个用户的兴趣比例不同，该方法不能简单的在用户之间进行平均，获得聚合度量 公平性（Fairness）：本篇文章中不考虑人之间的公平性，而是用户兴趣之间的公平性，也就是用正确的比例关系反应出来为什么本篇文章认为校准标准（calibration criteria）在公平性中尤其重要： 校准、等可能性、等机会不能同时满足，除非两种情况：机器学习做出完美预测（在现实世界中不可能）；不同组有相同的基本比率，即分类标签比例相同，通常在真实世界中也不可能出现。假设用户播放电影中70%是“浪漫类”，30%是“动作类”，那么这两种类型的基础比率很明显不同，两种类型电影的预测分数也会不同，因此，等可能性、等机会这样的公平性标准不能立即起适用 EXPERIMENTS上述图标也反映了，经过校准后的推荐列表中的推荐项仍然是用户感兴趣领域的电影，因此有必要进行扩展校准（extended calibration probability），将用户兴趣之外的电影也加入推荐当中计算推荐列表中50部电影的类型概率和用户播放历史的类型概率之间的差异，然后对所有用户求平均值，区分差值的正负值上图中，所有用户播放历史的类型概率的平均值是基准点，平均差值的正负决定了线段的长度，如果是完美推荐，那么线段长度应该为0。如果下面的线段比上方的线段长，那么这种类型电影的推荐与用户兴趣相比是过少的；如果上下一样长，那么这种类型电影的推荐在所有测试用户上的平均推荐比例是正确的（此外，上下方线段的长度还暗示了与每个用户兴趣比例的平均差距），要明确，推荐应该为每个用户都进行标准化，因此期望较小的线段长度Documentary、IMAX、Musical、Western是under-represented（最下端接近0）Action、Adventure、Crime、Mystery、Sci-Fi是over-represented（上端比下端长）上图采用λ=0.99的校准度 CONCLUSION本文是基于用户的角度，未来的研究工作将基于item的角度，比如一个item的推荐频率是否是另一个item的两倍，那这个 item很可能被选中（consume）的次数也是两倍的 论文原文Calibrated Recommendations]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>RecSys2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Soul产品分析报告]]></title>
    <url>%2F2018%2F07%2F26%2FSoul%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[产品概述产品信息产品名称：Soul版本：V3.0.12-18081801Slogan：跟随灵魂找到你简介：时下超火的弱关系社交，通过完成 30 秒的“灵魂测试”，来找到心灵相通的小伙伴。更有“Soul 荐”算法，秒推最能懂你的陌生人，随时随地聊天分享~还可能找到 Soulmate 产品特色灵魂匹配：通过简单的性格测试和各种深入测试将用户划分为不同类型的群体，称之为星球，为用户推荐性格相近的好友，弱化了看脸的社交本质，更注重灵魂相近、趣味相投的内在共鸣。 匿名社交：为用户提供一个良好的发泄内心的不快与郁结的平台，将用户关注的重点从某个人转移到某件事情本身，评论更加客观，让用户更愿意更放心抒发情绪，大大减少社交压力。 用户分析性别分布：女性略多于男性，主要原因分析如下：1) 在现实社交中，女性相比于男性有很大一部分的压力是来源于外表的评价。而 Soul 这款应用的定位是基于灵魂的社交，目的是让用户抛开外表的因素，更注重内涵，这可以让女性从看脸的社会中脱离出来，是释放压力的一个选择。2) 女性相比男性更加感性，更希望找人倾诉自己内心想法、不快，而匿名的平台会给人安全感，让她们在倾诉时更放松更安心。3) 小清新的视觉效果和内容风格更符合女性的审美。无论是黑白加薄荷绿的配色，还是整体氛围以及用户动态的风格都很干净清爽。 年龄分布：绝大部分用户集中在 35 岁以下，41 岁以上的人群所占比重最少，主要原因分析如下：1) 25-30岁的比例最高，这是大学生和刚入职的年轻人集中的年龄段，他们刚步入社会，会面多方面的压力，需要一个宣泄的平台，Soul 的匿名功能很好的解决了这部分需求。2) 25-30岁的年轻人精力和时间充沛，更喜欢尝试好玩有趣的事物，Soul基于灵魂匹配的社交富有创意和新鲜感，这一点会很吸引他们。3) 25-35岁是父母催婚人群的集中年龄段，由于网络的便利和工作的繁忙，社交软件是很多人寻求伴侣的选择，Soul 基于灵魂的交友也很戳中他们的痛点。 用户需求1、寻找更多与自己性格相近、兴趣相同的人需求分析：在实际生活中，很难找到真正性格相近、兴趣相同的“知音”，人们对认同感和灵魂共鸣的需求在现实生活中无法得到充分满足。 解决方案：灵魂匹配的功能。根据性格测试结果匹配好友，推荐给用户可能合得来的人，同时用户可以在自己的主页展示自我，进而帮助用户找到志趣相投的人。 2、自我表达，倾诉心声需求分析：自身有很多想法或烦恼，但是由于现实环境等因素，不能或不愿告知身边的人，寻求倾听者倾诉心声。 解决方案：匿名聊天功能。一对一聊天加上匿名功能，营造一个相对私密的环境，给用户安全感，满足用户想倾诉心声又不想秘密被泄露的需求。 3、释放压力，放松心灵需求分析：在生活中，由于工作的压力、家庭的负担、社交的不快等各种负面情绪的堆积，会导致人们内心焦躁、效率降低等，形成恶性循环，需要一个宣泄的途径。 解决方案：匿名发布瞬间。每个用户的瞬间都能发布在广场上，获得更多用户的安慰和支持，满足用户放松心灵寻求安慰的需求。，而且是匿名的，用户不必担心被认出引起不快，满足用户释放压力畅所欲言的需求。 4、缓解孤独，寻求陪伴需求分析：25-35 岁的人群占据了 Soul 用户相当大的比例，这个年龄段的人正需要解决单身问题，父母的催促和自身的孤独，使他们想寻求陪伴。 解决方案：“soulmate”图标点亮的功能。用户通过不断的聊天来积攒亲密度，亲密度用“soulmate”图标的点亮程度来展示，对点亮图标的期待使用户坚持的聊下去，进而实现用户关系的长久建立，满足用户寻求陪伴的需要。 5、展现自我，获得关注需求分析：Soul 的主要用户是学生党和年轻人，这个年龄段的人群有很强的表现欲，希望展现自己的性，获得认同。 解决方案：展现自我的广场和关注功能。用户将原创内容发布在广场上，所有人可以见，庞大的用户数量满足用户展现自我和想获取更多关注的需求。 产品分析产品结构图 主要功能1、星球 – 推荐人列表、匹配好友亮点：1) 推荐列表以球形展示，加上深蓝色的背景和自转的动画，很符合“星球”的概念；转动星球查看所有推荐用户，增添趣味性。2) 筛选条件除了年龄、性别，还包括星座。年轻人会更多地关注与星座相关的因素，符合 Soul 绝大多数用户的需求。3) 球形推荐列表中只显示个性签名和匹配度。信息简单却精确反应了用户最关心的信息：个性签名会反应一个人的性格特征，匹配度便于用户筛选；此外还特别注明了最匹配和最新人，是用户比较关心的信息。4) 普通匹配与语音匹配，玩法多样。 不足：1) 摇动手机切换星球时，会发现列表中的许多用户没有改变，需多次摇动手机才能看到更多的推荐用户。2) 语音匹配只有在通话期间有关注的按钮，如果用户没有注意到，或者忘记关注，在语音结束后就无法加好友。 2、广场 – 瞬间展示亮点：1) 置顶瞬间。系统根据点赞和评论人数进行推荐的瞬间，每次刷新都有新推荐，可以时刻发现最热门的话题。2) 发布的瞬间只会显示用户所在星球。一方面是为了匿名，另一方面根据用户所属星球也可以了解该用户大致的性格特质，便于用户寻找与自己所属同一星球的其他用户。3) 推荐音频悬浮图标的删除采用 Android 系统删除 App 的形式，简单易懂，减少了用户的学习成本。4) 右下角的按钮在下拉和静止时是通知消息，可以在浏览广场时随时查看评论、回复和点赞消息；在上拉时变为回到顶部，节省页面空间。5) 广场中瞬间的三个分类：推荐、关注和最新：推荐：系统推荐比较优质的内容；关注：感兴趣的用户发布的瞬间，让用户不会错过关注的人的动态；最新：最新最及时的用户消息，间接地告诉用户这些人是“在线”的。6) 可以删除其他用户的瞬间不显示，表示对这一类的瞬间内容无感，便于系统更精确地把握用户喜好，使推荐内容更符合用户口味。 不足：没有显示对瞬间的评论，需要点开瞬间查看详情才能看到其他用户的评论。不仅仅是瞬间的内容，有时评论也可以激起用户的热情，可以根据点赞筛选出热门评论和瞬间一起发布在广场页面上，增进用户间的互动。 3、聊天 – 好友互动亮点：1) 状态可选：想要聊天，允许匹配，拒绝匹配。当用户想要聊天时，很可能主动找其他用户聊天，这时进行用户推荐可以增进互动；当允许匹配时，可以将用户推荐给其他用户，增加被关注的几率；当用户拒绝匹配时，可以避免被打扰。2) 亲密度的显示，即“soulmate”图标的点亮程度。随着聊天时间的增长，图标会逐渐被点亮，在这个过程中，用户会对点亮图标抱有期待，会坚持和好友聊天，提高了用户的留存率和使用时长。 不足：消息显示已读（气泡旁的小绿点），当用户发现对方已读不回时，会增加用户的负面感受。 4、发布瞬间亮点：1) 特色功能：共创音频。可以增进用户之间的互动。而且原创者在发布语音的可选择是否允许共创，避免了出现其他用户不经允许滥用他人音频情况的发生。2) 多种权限可供选择，维护原创者的权利。3) 可添加标签，包括推荐标签和自定义标签。可以作为瞬间的分类要素，用户可以根据标签进行快捷搜索，找到更多有相同标签的瞬间。 不足：添加状态权限的按钮不明显，也没有文字提示，导致许多用户会忽略这个功能。 5、趣味测试优点：1) 测试过程中：一题一屏，消减用户看到题目较多时的厌烦情绪，还可以让用户的注意力集中在一道题目上，提高测试的准确度；显示测试进度，减少用户在题目层出不穷时的不耐烦。2) 测试结束后有对题目准确性的评价，可以为其他用户提供一个选择的参考标准。3) 趣味测试的功能以及其内容的定时更新，一方面满足了用户的新鲜感，是很多年轻人都喜欢的一类娱乐消遣；另一方面可以成为用户间交流的话题，增加用户间的互动。 不足：已做测试与未做测试混杂在一起，且“已测试过”和“未测试过”的标注不显眼。用户在选择测试时大多情况下不会按顺序选择，导致已做测试和未做测试混杂在一起，而且已做过的测试多数情况下用户是不想再做一遍的，会给用户的筛选造成干扰。个人觉得可以将趣味测试分为两栏：已做和未做，便于用户筛选。 总结Soul 主打灵魂匹配，注重内在而忽略外表的理念很吸引眼球，也是新用户选择 Soul 的主要原因；玩法多样新奇，星球、测试、瞬间、亲密度等，可以让用户在短期内保持新鲜感。但是也存在不足的方面，主要有以下三点： 一、未能真正做到灵魂交友Soul 给人的整体感觉是较为舒适清爽的，不像其他交友软件那样鱼龙混杂，但广场上依然存在用户发布自拍的现象，违背了灵魂交友的主题，会让许多用户感到反感。个人有两点建议：一是优化推荐算法，当检测到用户多次删除掉包含自拍的瞬间后，不再向用户推荐包含自拍的瞬间；另外一点是将是否看自拍的选择权交给用户，为包含自拍的瞬间打上标签，用户可选择是否屏蔽。 二、破冰的互动有所欠缺，尬聊依然存在。Soul 希望通过测试结果匹配好友的方法让用户之间的互动更加密切，但在现实的体验中，尬聊依然存在。与陌生人之间的交流需要话题的支撑，而两个都很内向、缺乏社交技巧、不会寻找话题的用户进行交流，势必会产生尬聊的局面，这时最应该做的是打破这个僵局，显然在这点上 Soul 做的还不够。这也是导致老用户流失的主要原因。个人建议增加一些破冰的互动，比如在聊天界面增加一个问答的功能，系统随机出一道趣味题目，一方面为交流增加话题，另一方面通过问答增进相互之间的了解，两个人了解的多了，话题自然而然就会变多，从而达到破冰的目的。 三、服务器不稳定服务器不稳定，经常崩溃是在 Soul 广场上经常能看到的吐槽，也带给用户及其不友好的体验。]]></content>
      <categories>
        <category>产品经理</category>
      </categories>
      <tags>
        <tag>产品分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的HTTP代理服务器的实现]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%AE%80%E5%8D%95%E7%9A%84HTTP%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD-%E6%8B%93%E5%B1%95%E5%8A%9F%E8%83%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[实现功能1、在指定端口（例如 8080）接收来自客户的 HTTP 请求并且根据其中的 URL 地址访问该地址所指向的 HTTP 服务器（原服务器），接收 HTTP 服务器的响应报文，并将响应报文转发给对应的客户进行浏览2、支持 Cache 功能：求能缓存原服务器响应的对象，并能够通过修改请求报文（添加 If-Modified-Since 头行），向原服务器确认缓存对象是否是最新版本3、网站过滤：允许/不允许重点内容访问某些网站4、用户过滤：支持/不支持某些用户访问外部网站5、网站引导：将用户对某个网站的访问引导至一个模拟网站（钓鱼） 完整的代码（VS2015）：https://github.com/Salanghei/ProxyServer 实现步骤1、加载套接字库，创建套接字2、初始化套接字：设置 IP 地址和端口等属性3、代理服务器接收客户端发送的 TCP 请求报文，并解析 HTTP 头部（method, url, host 等信息）4、拓展功能：网站过滤，用户过滤，网站引导，检查是否有本地缓存（改造HTTP头添加 If-Modified-Since 字段）5、代理连接到服务器，并将客户端发送的 HTTP 数据报文转发给目标服务器6、接收目标服务器的响应报文 ，解析报文头信息：状态码为304时不需要更新缓存，并将本地缓存转发给客户端；否则直接将响应报文转发给客户端，并进行缓存7、关闭套接字 关于程序的运行为了使浏览器访问网址时通过代理服务器，必须进行相关设置，以 Chrome 浏览器设置为例，打开 Chrome 浏览器中的设置，点击高级，选择打开代理设置：选择局域网设置，按如图所示进行配置 然后程序就可以监听10240端口啦~ 代码是根据指导书改编的，菜鸟一枚，还望大神们多多指教~]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程服务器 ubuntu16.04 caffe+matlab r2014b+python+cpu配置(三)]]></title>
    <url>%2F2018%2F02%2F18%2F%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8-ubuntu16-04-caffe-matlab-r2014b-python-cpu-only%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Caffe配置过程中的一些问题 参考链接：https://groups.google.com/forum/#!topic/caffe-users/x8Y26vf-or8http://blog.csdn.net/striker_v/article/details/52890747http://www.echojb.com/cuda/2017/05/05/375103.htmlhttp://www.caffecn.cn/?/question/1113https://blog.csdn.net/dym755833564/article/details/77965966 GCC版本太新在make过程中可能 string.h ‘memcy’ was not declared in this scope的错误，这是因为ubuntu中gcc编译器版本太新，解决方法是打开Makefile文件，找到下面这行代码（大概在409行左右）： 1NVCCFLAGS += -ccbin=(CXX)−Xcompiler−fPIC(COMMON_FLAGS) 将其改为： 1NVCCFLAGS += -D_FORCE_INLINES -ccbin=(CXX)−Xcompiler−fPIC(COMMON_FLAGS) make matcaffe时报错12345678910111213141516171819202122232425/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp: In function ‘void delete_solver(int, mxArray**, int, const mxArray**)’:/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:208:3: warning: lambda expressions only available with -std=c++11 or -std=gnu++11 [enabled by default]/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:208:4: error: no matching function for call to ‘remove_if(std::vector&lt;boost::shared_ptr&lt;caffe::Solver&lt;float&gt; &gt; &gt;::iterator, std::vector&lt;boost::shared_ptr&lt;caffe::Solver&lt;float&gt; &gt; &gt;::iterator, delete_solver(int, mxArray**, int, const mxArray**)::&lt;lambda(const boost::shared_ptr&lt;caffe::Solver&lt;float&gt; &gt;&amp;)&gt;)’/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:208:4: note: candidate is:In file included from /usr/include/c++/4.7/algorithm:63:0, from ./include/caffe/blob.hpp:4, from ./include/caffe/caffe.hpp:7, from /home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:18:/usr/include/c++/4.7/bits/stl_algo.h:1166:5: note: template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:208:4: error: template argument for ‘template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)’ uses local type ‘delete_solver(int, mxArray**, int, const mxArray**)::&lt;lambda(const boost::shared_ptr&lt;caffe::Solver&lt;float&gt; &gt;&amp;)&gt;’/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:208:4: error: trying to instantiate ‘template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)’/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp: In function ‘void delete_net(int, mxArray**, int, const mxArray**)’:/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:293:3: warning: lambda expressions only available with -std=c++11 or -std=gnu++11 [enabled by default]/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:293:4: error: no matching function for call to ‘remove_if(std::vector&lt;boost::shared_ptr&lt;caffe::Net&lt;float&gt; &gt; &gt;::iterator, std::vector&lt;boost::shared_ptr&lt;caffe::Net&lt;float&gt; &gt; &gt;::iterator, delete_net(int, mxArray**, int, const mxArray**)::&lt;lambda(const boost::shared_ptr&lt;caffe::Net&lt;float&gt; &gt;&amp;)&gt;)’/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:293:4: note: candidate is:In file included from /usr/include/c++/4.7/algorithm:63:0, from ./include/caffe/blob.hpp:4, from ./include/caffe/caffe.hpp:7, from /home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:18:/usr/include/c++/4.7/bits/stl_algo.h:1166:5: note: template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:293:4: error: template argument for ‘template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)’ uses local type ‘delete_net(int, mxArray**, int, const mxArray**)::&lt;lambda(const boost::shared_ptr&lt;caffe::Net&lt;float&gt; &gt;&amp;)&gt;’/home/zkk/caffe/matlab/+caffe/private/caffe_.cpp:293:4: error: trying to instantiate ‘template&lt;class _FIter, class _Predicate&gt; _FIter std::remove_if(_FIter, _FIter, _Predicate)’Makefile:518: recipe for target &apos;matlab/+caffe/private/caffe_.mexa64&apos; failedmake: *** [matlab/+caffe/private/caffe_.mexa64] Error 255 修改Makefile里面的CXXFLAGS，添加CXXFLAGS += -std=c++11.然后重新编译就可以了，即： 12CXXFLAGS += -MMD -MPCXXFLAGS += -std=c++11 注意： 是添加不是修改 make mattest时报错：123456789Invalid MEX-file &apos;caffe/matlab/+caffe/private/caffe_.mexa64&apos;:caffe/matlab/+caffe/private/caffe_.mexa64: undefinedsymbol: _ZN2cv8imencodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKNS_11_InputArrayERSt6vectorIhSaIhEERKSB_IiSaIiEE。 Error in caffe.set_mode_cpu (line 5)caffe_(&apos;set_mode_cpu&apos;); Error in caffe.run_tests (line 6)caffe.set_mode_cpu(); 有些说法是要执行如下命令： 12export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4:/usr/lib/x86_64-linux-gnu/libopencv_imgproc.so.2.4:/usr/lib/x86_64-linux-gnu/libopencv_core.so.2.4:/usr/lib/x86_64-linux-gnu/libstdc++.so.6:/usr/lib/x86_64-linux-gnu/libfreetype.so.6export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 但是再次运行make mattest时会有新错误： 1234malloc: unknown:0: assertion botchedfree: called with unallocated block argumentlast command: (null)Aborting...find: ‘bash’ terminated by signal 6 虽然最后还是能运行成功，但发现在运行bash指令时无法成功执行，纠结了好久发现是这里没有彻底解决问题，解决方法是： 将 libopencv_imgproc.so.2.4, libopencv_core.so.2.4, libopencv_highgui.so.2.4三个文件拷贝到/usr/local/MATLAB/R2016b/bin/glnxa64/中 12345678$ sudo mv /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_imgproc.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_imgproc.so.2.4.old$ sudo cp /usr/lib/x86_64-linux-gnu/libopencv_imgproc.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/$ sudo mv /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_core.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_core.so.2.4.old$ sudo cp /usr/lib/x86_64-linux-gnu/libopencv_core.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/$ sudo mv /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_highgui.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/libopencv_highgui.so.2.4.old$ sudo cp /usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4 /usr/local/MATLAB/R2016b/bin/glnxa64/ 再执行 1export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 问题就解决啦，bash指令也没有问题啦 关于opencvmake all时报错，错误信息： 1234567...CXX/LD -o .build_release/tools/convert_imageset.bin .build_release/lib/libcaffe.so: undefined reference to cv::imread(cv::String const&amp;, int)’ .build_release/lib/libcaffe.so: undefined reference to cv::imencode(cv::String const&amp;, cv::_InputArray const&amp;, std::vector &gt;&amp;, std::vector &gt; const&amp;)’ .build_release/lib/libcaffe.so: undefined reference to cv::imdecode(cv::_InputArray const&amp;, int)’ collect2: error: ld returned 1 exit status ...Makefile:518: recipe for target &apos;matlab/+caffe/private/caffe_.mexa64&apos; failedmake: *** [matlab/+caffe/private/caffe_.mexa64] Error 255 首先，已经配置过了opencv，可以这样查询安装版本： 1$ pkg-config --modversion opencv 所以出现上面的错误，应该是opencv_imgcodecs链接的问题，比较有效的解决方案是，把opencv需要的lib添加到Makefile文件中，找到LIBRARIES（在PYTHON_LIBRARIES := boost_python python2.7 前一行），并进行修改，向该句末尾添加： 1opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs 我的Makefile文件修改后为： 1LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程服务器 ubuntu16.04 caffe+matlab r2014b+python+cpu配置(二)]]></title>
    <url>%2F2018%2F02%2F18%2F%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8-ubuntu16-04-caffe-matlab-r2014b-python-cpu-only%E9%85%8D%E7%BD%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[配置Caffe+Matlab+Python（CPU-ONLY） 本文参考文章：ubuntu16.04, Matlab2016b caffe编译安装：https://www.cnblogs.com/haoliuhust/p/7738920.html官方安装文档：http://caffe.berkeleyvision.org/installation.htmlUbuntu16.04安装Caffe(CPU Only)：http://blog.csdn.net/muzilinxi90/article/details/53673184 安装依赖项1234$ sudo apt update$ sudo apt search openblas$ sudo apt install libopenblas-dev$ sudo update-alternatives --config libblas.so.3 12$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler$ sudo apt-get install --no-install-recommends libboost-all-dev 1$ sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 此外还要安装git，cmake 12$ sudo apt-get install git$ sudo apt-get install cmake 下载Caffe源码12$ git clone https://github.com/BVLC/caffe.git$ cd caffe/ python 依赖库 1234$ cd python/$ apt-get install python-pip $ pip install --upgrade pip$ for req in $(cat requirements.txt); do pip install $req; done 编译Caffe在Caffe文件夹下，拷贝一份Makefile.config.example并重命名成Makefile.config，修改该配置文件： 1$ cp Makefile.config.example Makefile.config 打开Makefile.config（vi Makefile.config），因为这里没有配置GPU，所以去掉CPU_ONLY := 1前面的注释，MATLAB_DIR后改为matlab的安装地址 由于Ubuntu16.04文件结构的变化，#Whatever else you find you need goes here.处要改成下面这样： 123# Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial 注意： 每条路径之间一定要有空格，否则仍然会报错 接下来编译Caffe 123$ make all$ make test$ make runtest 编译python接口1$ make pycaffe 进入caffe/python，打开python，输入import caffe，如果没有报错则配置成功 编译matlab接口12$ make matcaffe$ make mattest]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程服务器 ubuntu16.04 caffe+matlab r2014b+python+cpu配置(一)]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8-ubuntu16-04-caffe-matlab-r2014b-python-cpu-only%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[远程服务器Ubuntu16.04 安装Matlab R2014b 本文参考链接：http://blog.csdn.net/wangpengfei163/article/details/47311041 所需资源下载：链接: https://pan.baidu.com/s/1bqotI03 密码: w28s本文中只涉及到libmwservices.so.rar，MATLAB_R2014B_MAC_LINUX_crack.zip，R2014b_glnxa64.iso三个文件。 替换install.jar文件将R2014b_glnxa64.iso/java/jar/install.jar替换为MATLAB_R2014B_MAC_LINUX_crack.zip中的install.jar文件 可以使用UltraIso工具（试用版仅支持100M以下文件的改动，注册码请自行百度） 上传文件到服务器应该是个笨方法吧，实在不清楚在远程服务器上应该怎么挂载镜像了【捂脸】 使用scp命令上传文件：scp local_file remote_username@remote_ip:remote_folder 上传文件的过程有点漫长，6G的文件传了一晚上。。。 挂载镜像建议使用root用户，文件挂载命令格式如下： 1mount -o loop,rw /home/R2014b_glnxa64.iso /matlab 其中/matlab是要挂载的文件夹目录，建议新建一个文件夹。 安装Matlab在SSH上远程安装采用第二种方式，输入如下命令： 1# ./install -mode silent -fileInstallationKey 29797-39064-48306-32452 -agreeToLicense yes -activationPropertiesFile /home/activate.ini -destinationFolder /usr/local/MATLAB/R2014b 其中activate.ini文件需要按照文件内容进行设置，其中的license.lic是MATLAB_R2014B_MAC_LINUX_crack.zip中的文件，需将其解压，路径自行设置 12activateCommand=activateOfflinelicenseFile=/home/license.lic 如果未成功激活，可以在安装结束后使用如下命令激活，也需要设置activate.ini 1#/usr/local/MATLAB/R2014b/bin/activate_matlab.sh -propertiesFile /home/activate.ini 破解Matlab将libmwservices.so.rar中的so文件拷贝到/usr/local/MATLAB/R2014b/bin/glnxa64/文件夹下 12rm -rf /usr/local/MATLAB/R2014b/bin/glnxa64/libmwservices.socp /home/libmwservices.so /usr/local/MATLAB/R2014b/bin/glnxa64/ 安装成功，运行/usr/local/MATLAB/R2014b/bin/matlab 配置环境变量1vi /etc/profile 在文件末尾添加 1export PATH=/usr/local/MATLAB/R2014b/bin:$PATH 保存并退出后使设置生效 1source /etc/profile 此时直接输入matlab即可启动软件]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web的新浪云SAE部署]]></title>
    <url>%2F2017%2F10%2F10%2FJava-Web%E7%9A%84%E6%96%B0%E6%B5%AA%E4%BA%91SAE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[创建应用在新浪云注册后即可在SAE平台上创建应用，在云平台SAE中点击 应用管理 &gt; 创建新应用，选择相应的部署环境和云空间配置，填写好应用信息，其中应用名称一般与项目名称一致： 进入管理应用页面，总览中显示的二级域名即为网站的网址，Access Key为数据库连接时的用户名，Secret Key为密码： 配置数据库点击 数据库与缓存服务 &gt; 共享型MySQL，点击创建MySQL创建数据库，点击管理选项可进入数据库的管理页面，点击SQL选项卡，即可对数据库进行各种操作，与MySQL的命令行类似，可以通过命令建表，也可以导入数据表： 数据库连接在项目中要做好相应修改，相关代码在官方文档中已经给出： 只需将数据库连接修改为： 123456789101112String driver = &quot;com.mysql.jdbc.Driver&quot;;String username = System.getenv(&quot;ACCESSKEY&quot;);String password = System.getenv(&quot;SECRETKEY&quot;);//System.getenv(&quot;MYSQL_HOST_S&quot;); 为从库，只读String dbUrl = String.format(&quot;jdbc:mysql://%s:%s/%s&quot;, System.getenv(&quot;MYSQL_HOST&quot;), System.getenv(&quot;MYSQL_PORT&quot;), System.getenv(&quot;MYSQL_DB&quot;));try &#123; Class.forName(driver).newInstance(); con = DriverManager.getConnection(dbUrl, username, password); // ...&#125; catch (Exception e) &#123; // ...&#125; 修改后将项目导出为war包，点击 应用 &gt; 代码管理，将导出的war包上传。至此，云平台部署完成，通过二级域名即可访问。]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下在Java中调用GraphViz]]></title>
    <url>%2F2017%2F07%2F23%2FWindows%E4%B8%8B%E5%9C%A8Java%E4%B8%AD%E8%B0%83%E7%94%A8GraphViz%2F</url>
    <content type="text"><![CDATA[相关文章及代码How to call GraphViz from java：https://stackoverflow.com/questions/26481910/how-to-call-graphviz-from-javajava – java中调用GraphViz：http://blog.csdn.net/TheSnowBoy_2/article/details/52540874windows下Graphviz安装及入门教程：http://blog.csdn.net/lanchunhui/article/details/49472949 GraphViz环境变量的配置安装好GraphViz后，将Graphviz安装目录下的bin文件夹添加到Path环境变量中： 验证：进入命令行界面，输入dot -version回车，会显示GraphViz相关的版本信息，则配置成功： 为GraphViz的java的API建立配置文件在工程目录下创建test文件夹，在test文件夹下创建tmpDir文件夹，此文件夹用来存放临时文件。再在工程目录下创建config文件夹，并将文件config.properties放在该文件夹内。 config.properties内容如下： 1234567############################################################### Windows Configurations ################################################################ The dir. where temporary files will be created.tempDirForWindows10 = E:/temp# Where is your dot program located? It will be called externally.dotForWindows10 = &quot;E:/software_daily/FOR_LEARN/graphViz/bin/dot.exe&quot; 将其中tempDirForWindows10和dotForWindows10中的Windows10改为相应的Windows版本，dotForWindows10后的路径改为dot.exe的安装路径。 ###GraphViz的java API源代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337import java.io.BufferedReader;import java.io.BufferedWriter;import java.io.DataInputStream;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.FileWriter;import java.io.InputStreamReader;import java.util.Properties;/*** &lt;dl&gt;* &lt;dt&gt;Purpose: GraphViz Java API* &lt;dd&gt;** &lt;dt&gt;Description:* &lt;dd&gt; With this Java class you can simply call dot* from your Java programs.* &lt;dt&gt;Example usage:* &lt;dd&gt;* &lt;pre&gt;* GraphViz gv = new GraphViz();* gv.addln(gv.start_graph());* gv.addln(&quot;A -&gt; B;&quot;);* gv.addln(&quot;A -&gt; C;&quot;);* gv.addln(gv.end_graph());* System.out.println(gv.getDotSource());** String type = &quot;gif&quot;;* File out = new File(&quot;out.&quot; + type); // out.gif in this example* gv.writeGraphToFile( gv.getGraph( gv.getDotSource(), type ), out );* &lt;/pre&gt;* &lt;/dd&gt;** &lt;/dl&gt;** @version v0.5.1, 2013/03/18 (March) -- Patch of Juan Hoyos (Mac support)* @version v0.5, 2012/04/24 (April) -- Patch of Abdur Rahman (OS detection + start subgraph + * read config file)* @version v0.4, 2011/02/05 (February) -- Patch of Keheliya Gallaba is added. Now you* can specify the type of the output file: gif, dot, fig, pdf, ps, svg, png, etc.* @version v0.3, 2010/11/29 (November) -- Windows support + ability to read the graph from a text file* @version v0.2, 2010/07/22 (July) -- bug fix* @version v0.1, 2003/12/04 (December) -- first release* @author Laszlo Szathmary (&lt;a href=&quot;jabba.laci@gmail.com&quot;&gt;jabba.laci@gmail.com&lt;/a&gt;)*/public class GraphViz&#123; /** * Detects the client&apos;s operating system. */ private final static String osName = System.getProperty(&quot;os.name&quot;).replaceAll(&quot;\\s&quot;,&quot;&quot;); /** * Load the config.properties file. */ private final static String cfgProp = &quot;config/config.properties&quot;; private final static Properties configFile = new Properties() &#123; private final static long serialVersionUID = 1L; &#123; try &#123; load(new FileInputStream(cfgProp)); &#125; catch (Exception e) &#123;&#125; &#125; &#125;; /** * The dir. where temporary files will be created. */private static String TEMP_DIR = &quot;test/tmpDir&quot;; /** * Where is your dot program located? It will be called externally. */private static String DOT = configFile.getProperty(&quot;dotFor&quot; + osName); /** * The image size in dpi. 96 dpi is normal size. Higher values are 10% higher each. * Lower values 10% lower each. * * dpi patch by Peter Mueller */ private int[] dpiSizes = &#123;46, 51, 57, 63, 70, 78, 86, 96, 106, 116, 128, 141, 155, 170, 187, 206, 226, 249&#125;; /** * Define the index in the image size array. */ private int currentDpiPos = 7; /** * Increase the image size (dpi). */ public void increaseDpi() &#123; if ( this.currentDpiPos &lt; (this.dpiSizes.length - 1) ) &#123; ++this.currentDpiPos; &#125; &#125; /** * Decrease the image size (dpi). */ public void decreaseDpi() &#123; if (this.currentDpiPos &gt; 0) &#123; --this.currentDpiPos; &#125; &#125; public int getImageDpi() &#123; return this.dpiSizes[this.currentDpiPos]; &#125; /** * The source of the graph written in dot language. */ private StringBuilder graph = new StringBuilder(); /** * Constructor: creates a new GraphViz object that will contain * a graph. */ public GraphViz() &#123; &#125; /** * Returns the graph&apos;s source description in dot language. * @return Source of the graph in dot language. */ public String getDotSource() &#123; return this.graph.toString(); &#125; /** * Adds a string to the graph&apos;s source (without newline). */ public void add(String line) &#123; this.graph.append(line); &#125; /** * Adds a string to the graph&apos;s source (with newline). */ public void addln(String line) &#123; this.graph.append(line + &quot;\n&quot;); &#125; /** * Adds a newline to the graph&apos;s source. */ public void addln() &#123; this.graph.append(&apos;\n&apos;); &#125; public void clearGraph()&#123; this.graph = new StringBuilder(); &#125; /** * Returns the graph as an image in binary format. * @param dot_source Source of the graph to be drawn. * @param type Type of the output image to be produced, e.g.: gif, dot, fig, pdf, ps, svg, png. * @return A byte array containing the image of the graph. */ public byte[] getGraph(String dot_source, String type) &#123; File dot; byte[] img_stream = null; try &#123; dot = writeDotSourceToFile(dot_source); if (dot != null) &#123; img_stream = get_img_stream(dot, type); if (dot.delete() == false) System.err.println(&quot;Warning: &quot; + dot.getAbsolutePath() + &quot; could not be deleted!&quot;); return img_stream; &#125; return null; &#125; catch (java.io.IOException ioe) &#123; return null; &#125; &#125; /** * Writes the graph&apos;s image in a file. * @param img A byte array containing the image of the graph. * @param file Name of the file to where we want to write. * @return Success: 1, Failure: -1 */ public int writeGraphToFile(byte[] img, String file) &#123; File to = new File(file); return writeGraphToFile(img, to); &#125; /** * Writes the graph&apos;s image in a file. * @param img A byte array containing the image of the graph. * @param to A File object to where we want to write. * @return Success: 1, Failure: -1 */ public int writeGraphToFile(byte[] img, File to) &#123; try &#123; FileOutputStream fos = new FileOutputStream(to); fos.write(img); fos.close(); &#125; catch (java.io.IOException ioe) &#123; return -1; &#125; return 1; &#125; /** * It will call the external dot program, and return the image in * binary format. * @param dot Source of the graph (in dot language). * @param type Type of the output image to be produced, e.g.: gif, dot, fig, pdf, ps, svg, png. * @return The image of the graph in .gif format. */ private byte[] get_img_stream(File dot, String type) &#123; File img; byte[] img_stream = null; try &#123; img = File.createTempFile(&quot;graph_&quot;, &quot;.&quot;+type, new File(GraphViz.TEMP_DIR)); Runtime rt = Runtime.getRuntime(); // patch by Mike Chenault String[] args = &#123;DOT, &quot;-T&quot;+type, &quot;-Gdpi=&quot;+dpiSizes[this.currentDpiPos], dot.getAbsolutePath(), &quot;-o&quot;, img.getAbsolutePath()&#125;; Process p = rt.exec(args); p.waitFor(); FileInputStream in = new FileInputStream(img.getAbsolutePath()); img_stream = new byte[in.available()]; in.read(img_stream); // Close it if we need to if( in != null ) in.close(); if (img.delete() == false) System.err.println(&quot;Warning: &quot; + img.getAbsolutePath() + &quot; could not be deleted!&quot;); &#125; catch (java.io.IOException ioe) &#123; System.err.println(&quot;Error: in I/O processing of tempfile in dir &quot; + GraphViz.TEMP_DIR+&quot;\n&quot;); System.err.println(&quot; or in calling external command&quot;); ioe.printStackTrace(); &#125; catch (java.lang.InterruptedException ie) &#123; System.err.println(&quot;Error: the execution of the external program was interrupted&quot;); ie.printStackTrace(); &#125; return img_stream; &#125; /** * Writes the source of the graph in a file, and returns the written file * as a File object. * @param str Source of the graph (in dot language). * @return The file (as a File object) that contains the source of the graph. */ private File writeDotSourceToFile(String str) throws java.io.IOException &#123; File temp; try &#123; temp = File.createTempFile(&quot;dorrr&quot;,&quot;.dot&quot;, new File(GraphViz.TEMP_DIR)); FileWriter fout = new FileWriter(temp); fout.write(str); BufferedWriter br=new BufferedWriter(new FileWriter(&quot;dotsource.dot&quot;)); br.write(str); br.flush(); br.close(); fout.close(); &#125; catch (Exception e) &#123; System.err.println(&quot;Error: I/O error while writing the dot source to temp file!&quot;); return null; &#125; return temp; &#125; /** * Returns a string that is used to start a graph. * @return A string to open a graph. */ public String start_graph() &#123; return &quot;digraph G &#123;&quot;; &#125; /** * Returns a string that is used to end a graph. * @return A string to close a graph. */ public String end_graph() &#123; return &quot;&#125;&quot;; &#125; /** * Takes the cluster or subgraph id as input parameter and returns a string * that is used to start a subgraph. * @return A string to open a subgraph. */ public String start_subgraph(int clusterid) &#123; return &quot;subgraph cluster_&quot; + clusterid + &quot; &#123;&quot;; &#125; /** * Returns a string that is used to end a graph. * @return A string to close a graph. */ public String end_subgraph() &#123; return &quot;&#125;&quot;; &#125; /** * Read a DOT graph from a text file. * * @param input Input text file containing the DOT graph * source. */ public void readSource(String input) &#123; StringBuilder sb = new StringBuilder(); try &#123; FileInputStream fis = new FileInputStream(input); DataInputStream dis = new DataInputStream(fis); BufferedReader br = new BufferedReader(new InputStreamReader(dis)); String line; while ((line = br.readLine()) != null) &#123; sb.append(line); &#125; dis.close(); &#125; catch (Exception e) &#123; System.err.println(&quot;Error: &quot; + e.getMessage()); &#125; this.graph = sb; &#125; 使用示例通过调用API绘制一个简单的有向图 1234567891011121314151617public static void createDotGraph(String dotFormat,String fileName)&#123; GraphViz gv=new GraphViz(); gv.addln(gv.start_graph()); gv.add(dotFormat); gv.addln(gv.end_graph()); String type = &quot;jpg&quot;; //输出图文件的格式，以.jpg为例 gv.decreaseDpi(); gv.decreaseDpi(); File out = new File(fileName+&quot;.&quot;+ type); gv.writeGraphToFile( gv.getGraph( gv.getDotSource(), type ), out );&#125;public static void main(String[] args) throws Exception &#123; String dotFormat=&quot;1-&gt;2;1-&gt;3;1-&gt;4;4-&gt;5;4-&gt;6;6-&gt;7;5-&gt;7;3-&gt;8;3-&gt;6;8-&gt;7;2-&gt;8;2-&gt;5;&quot;; createDotGraph(dotFormat, &quot;DotGraph&quot;);&#125; 输出图为：]]></content>
      <categories>
        <category>踩过的坑</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
